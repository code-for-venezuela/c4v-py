{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brat Tags Data Analysis\n",
    "\n",
    "## IDs in Brat \n",
    "\n",
    "- T: text-bound annotation\n",
    "- R: relation\n",
    "- E: event\n",
    "- A: attribute\n",
    "- M: modification (alias for attribute, for backward compatibility)\n",
    "- N: normalization [new in v1.3]\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "### Annotation ID conventions\n",
    "All annotations IDs consist of a single upper-case character identifying the annotation type and a number. The initial ID characters relate to annotation types as follows:\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Entity annotations\n",
    "Each entity annotation has a unique ID and is defined by type (e.g. Person or Organization) and the span of characters containing the entity mention (represented as a \"start end\" offset pair).\n",
    "\n",
    "| ID \t| Type And Span      \t| Text     \t|\n",
    "|----\t|--------------------\t|----------\t|\n",
    "| T1 \t| Organization 0 4   \t| Sony     \t|\n",
    "| T3 \t| Organization 33 41 \t| Ericsson \t|\n",
    "| T3 \t| Country 75 81      \t| Sweden   \t|\n",
    "\n",
    "<br> \n",
    "\n",
    "#### Attribute and modification annotations\n",
    "Attribute annotations are binary or multi-valued \"flags\" that specify further aspects of other annotations. Attributes have a unique ID and are defined by reference to the ID of the annotation that the attribute marks and the attribute value.\n",
    "\n",
    "| ID \t| Type & Entity ID  \t|\n",
    "|----\t|-------------------\t|\n",
    "| A1 \t| Negation T1       \t|\n",
    "| A2 \t| Confidence T2     \t|\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Relation annotations\n",
    "Binary relations have a unique ID and are defined by their type (e.g. Origin, Part-of) and their arguments.\n",
    "\n",
    "| ID \t| Type and Args          \t|\n",
    "|----\t|------------------------\t|\n",
    "| R1 \t| Origin Arg1:T3 Arg2:T4 \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Iteration\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "- Hand-picked: These tweets were handpicked if they were good examples of true public services reports or if the tweet seemed to not be a report but it really was one (confusing tweets.\n",
    "\n",
    "- Sampled: Random sample of tweets was drawn from the original dataframe. The sampling process was done apart. \n",
    "\n",
    "<br>\n",
    "\n",
    "- Hand-picked\n",
    "\n",
    "\n",
    "| id_parsed \t|          annotation_parsed \t| Count \t|\n",
    "|----------:\t|---------------------------:\t|------:\t|\n",
    "| A         \t| without-service            \t| 63    \t|\n",
    "|           \t| location                   \t| 38    \t|\n",
    "|           \t| duration                   \t| 16    \t|\n",
    "|           \t| time                       \t| 10    \t|\n",
    "|           \t| fake-information           \t| 2     \t|\n",
    "|           \t| with-service               \t| 2     \t|\n",
    "|           \t| reason                     \t| 1     \t|\n",
    "| T         \t| circumstantial-information \t| 65    \t|\n",
    "|           \t| social-report              \t| 39    \t|\n",
    "|           \t| electricity                \t| 33    \t|\n",
    "|           \t| gasoline                   \t| 25    \t|\n",
    "|           \t| water                      \t| 6     \t|\n",
    "|           \t| gas                        \t| 4     \t|\n",
    "\n",
    "<br>\n",
    "\n",
    "- Sampled\n",
    "\n",
    "| id_parsed \t|          annotation_parsed \t| Freq \t|\n",
    "|----------:\t|---------------------------:\t|-----:\t|\n",
    "| A         \t| without-service            \t| 24   \t|\n",
    "|           \t| location                   \t| 24   \t|\n",
    "|           \t| time                       \t| 10   \t|\n",
    "|           \t| utility-company            \t| 5    \t|\n",
    "|           \t| duration                   \t| 4    \t|\n",
    "|           \t| fake-information           \t| 4    \t|\n",
    "|           \t| politician                 \t| 4    \t|\n",
    "|           \t| reason                     \t| 3    \t|\n",
    "|           \t| news-company               \t| 3    \t|\n",
    "|           \t| with-service               \t| 2    \t|\n",
    "|           \t| other                      \t| 1    \t|\n",
    "| T         \t| circumstantial-information \t| 41   \t|\n",
    "|           \t| electricity                \t| 26   \t|\n",
    "|           \t| social-report              \t| 25   \t|\n",
    "|           \t| twitter-account            \t| 12   \t|\n",
    "|           \t| gasoline                   \t| 3    \t|\n",
    "|           \t| water                      \t| 1    \t|\n",
    "|           \t| water                      \t| 1    \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:04:06.174956Z",
     "start_time": "2020-07-26T02:04:06.120734Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "\n",
    "### Read Data\n",
    "original_df = pd.read_csv('../data/processed/brat/balanced_dataset_brat.ann', sep = '\\t',header = None)\n",
    "\n",
    "## Deprecated path\n",
    "# ../brat-v1.3_Crunchy_Frog/data/first-iter/balanced_dataset_brat.ann\n",
    "\n",
    "# Rename coumns \n",
    "original_df.columns = ['id', 'annotation', 'text']\n",
    "\n",
    "# Remove the ID numbers to know if it's an entity (T) or Attribute (A)\n",
    "original_df['id_parsed'] = original_df.id.str.replace('\\d', '')\n",
    "\n",
    "# Remove text span and IDs (T & A) from column. This columns has the name of the attributes and etitites \n",
    "original_df['annotation_parsed'] = original_df.annotation.str.replace('[\\dTA]', '')\n",
    "\n",
    "\n",
    "# Remove Relation tags\n",
    "# Change Relation Id to Null\n",
    "original_df.id_parsed.replace('R', np.nan, inplace= True)\n",
    "\n",
    "# Remove nulls\n",
    "original_df.dropna(subset=['id_parsed'], inplace= True)\n",
    "\n",
    "# Group by id_parsed, annotation parsed and count results\n",
    "df = original_df[['id_parsed', 'annotation_parsed']].groupby(['id_parsed', 'annotation_parsed'], sort = True).agg({'annotation_parsed':['count']}).copy()\n",
    "\n",
    "# After the group by there's multi-index columns. We rename the columns to have the level that we want (count)\n",
    "df.columns = df.columns.levels[1]\n",
    "\n",
    "# sort_values by index. Here the trick is to also use sort_index!!\n",
    "df.sort_values('count', ascending=False)\\\n",
    "    .sort_index(level=[0], ascending=[True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampled\n",
    "\n",
    "The following data was randomly sampled with the helper function ``` data_sampler.py ``` \n",
    "\n",
    "- Random_state: 58 \n",
    "- Sample: 30 \n",
    "\n",
    "` python data_sampler.py 58 30 brat-v1.3_Crunchy_Frog/data/first-iter/sampled_58_30.txt `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:04:06.538092Z",
     "start_time": "2020-07-26T02:04:06.179824Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_df = pd.read_csv('../data/raw/tweets/tagging-set-original_for_jupyter_tagging.csv')\n",
    "ann = '../data/processed/brat/balanced_dataset_brat.ann'\n",
    "txt = '../data/processed/brat/balanced_dataset_brat.txt'\n",
    "# pd.DataFrame(complete_df.sample(30, random_state = 9).full_text)\n",
    "\n",
    "test_balance = pd.read_csv(ann, sep = '\\t')\n",
    "## Deprecated path\n",
    "# ../brat-v1.3_Crunchy_Frog/data/first-iter/balanced_dataset_brat.ann\n",
    "\n",
    "# Rename coumns \n",
    "test_balance.columns = ['id', 'annotation', 'text']\n",
    "\n",
    "# Remove the ID numbers to know if it's an entity (T) or Attribute (A)\n",
    "test_balance['id_parsed'] = test_balance.id.str.replace('\\d', '')\n",
    "\n",
    "# Remove text span and IDs (T & A) from column. This columns has the name of the attributes and etitites \n",
    "test_balance['annotation_parsed'] = test_balance.annotation.str.replace('[\\dTA]', '')\n",
    "\n",
    "\n",
    "# Remove Relation tags\n",
    "# Change Relation Id to Null\n",
    "test_balance.id_parsed.replace('R', np.nan, inplace= True)\n",
    "\n",
    "# Remove nulls\n",
    "test_balance.dropna(subset=['id_parsed'], inplace= True)\n",
    "\n",
    "# Group by id_parsed, annotation parsed and count results\n",
    "df = test_balance[['id_parsed', 'annotation_parsed']].groupby(['id_parsed', 'annotation_parsed'], sort = True).agg({'annotation_parsed':['count']}).copy()\n",
    "\n",
    "# After the group by there's multi-index columns. We rename the columns to have the level that we want (count)\n",
    "df.columns = df.columns.levels[1]\n",
    "\n",
    "# sort_values by index. Here the trick is to also use sort_index!!\n",
    "df.sort_values('count', ascending=False)\\\n",
    "    .sort_index(level=[0], ascending=[True])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data to input into baseline model\n",
    "\n",
    "Strategy:\n",
    "\n",
    "<br>\n",
    "\n",
    "- Volarme atributos y relations. \n",
    "- Picar dataframe en espacios, anotaicion, comienzo de span y final de span\n",
    "- Hacer Sort del Dataframe por la segunda clumna (comienzo de span)\n",
    "- Cargar el archivo original de texto, La primera columna es la posicion en el documento de inicio de la linea\n",
    "- Hacer join con el inicio de la linea. Schema - inicio de la linea y tweet. \n",
    "\n",
    "<br>\n",
    "\n",
    "Roadblock\n",
    "\n",
    "- The problem with this approach is that the text file's lines do not match with the annotation file's span. This is due that the span is for the tag, not necessarily the first word of the tweet is going to be tagged. The same is for the last tag's span vs the end of the string. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper Function\n",
    "\n",
    "This helper function reads brat's annotation file (.ann) and parses to a dataframe with the following schema: \n",
    "\n",
    "\n",
    "| Column            \t| Dtype  \t|\n",
    "|-------------------\t|--------\t|\n",
    "| id                \t| object \t|\n",
    "| annotation        \t| object \t|\n",
    "| text              \t| object \t|\n",
    "| id_parsed         \t| object \t|\n",
    "| annotation_parsed \t| object \t|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:04:06.570598Z",
     "start_time": "2020-07-26T02:04:06.541910Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def annotation_parser(dir_ann_file, print_grouped_annotations = False):\n",
    "    '''\n",
    "        Helper function to parse Brat's annotation file (.ann). \n",
    "            Returns a dataframe with the following schema: \n",
    "                | id                \t| object \t|\n",
    "                | annotation        \t| object \t|\n",
    "                | text              \t| object \t|\n",
    "                | id_parsed         \t| object \t|\n",
    "                | annotation_parsed \t| object \t|\n",
    "    --------------------------------------------------------------\n",
    "    \n",
    "    Params:\n",
    "        \n",
    "        dir_ann_file: String, Default = None. \n",
    "            Path to ann file.\n",
    "        \n",
    "        print_grouped_annotation: Bool. Default = False. \n",
    "            Prints count of Entities and Attributes.\n",
    "    '''\n",
    "    \n",
    "    ### Read Data\n",
    "    to_parse_df = pd.read_csv(dir_ann_file, sep = '\\t',header = None)\n",
    "\n",
    "    # Rename coumns \n",
    "    to_parse_df.columns = ['id', 'annotation', 'text']\n",
    "\n",
    "    # Remove the ID numbers to know if it's an entity (T) or Attribute (A)\n",
    "    to_parse_df['id_parsed'] = to_parse_df.id.str.replace('\\d', '')\n",
    "\n",
    "    # Remove text span and IDs (T & A) from column. This columns has the name of the attributes and etitites \n",
    "    to_parse_df['annotation_parsed'] = to_parse_df.annotation.str.replace('[\\dTA]', '')\n",
    "\n",
    "\n",
    "    # Remove Relation tags\n",
    "    # Change Relation Id to Null\n",
    "    to_parse_df.id_parsed.replace('R', np.nan, inplace= True)\n",
    "\n",
    "    # Remove nulls\n",
    "    to_parse_df.dropna(subset=['id_parsed'], inplace= True)\n",
    "\n",
    "    # Group by id_parsed, annotation parsed and count results\n",
    "    df = to_parse_df[['id_parsed', 'annotation_parsed']].groupby(['id_parsed', 'annotation_parsed'], sort = True).agg({'annotation_parsed':['count']}).copy()\n",
    "\n",
    "    # After the group by there's multi-index columns. We rename the columns to have the level that we want (count)\n",
    "    df.columns = df.columns.levels[1]\n",
    "\n",
    "    # sort_values by index. Here the trick is to also use sort_index!!\n",
    "    \n",
    "    if print_grouped_annotations == True:\n",
    "    \n",
    "        print(df.sort_values('count', ascending=False)\\\n",
    "            .sort_index(level=[0], ascending=[True]))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return to_parse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Annotations\n",
    "\n",
    "<br>\n",
    "\n",
    "- Used annotation_parser helper function to read the data. \n",
    "- Subset only Entities for simplicity sake. \n",
    "- Create span columns and store it into data frame split_ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:04:08.625667Z",
     "start_time": "2020-07-26T02:04:08.571680Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Read sampled data\n",
    "sampled_ann = annotation_parser(ann) # 308 rows \n",
    "\n",
    "# # Deprecated path \n",
    "# ../brat-v1.3_Crunchy_Frog/data/first-iter/balanced_dataset_brat.ann\n",
    "\n",
    "# Subset Entities and rewrite dataframe\n",
    "sampled_ann = sampled_ann[sampled_ann.id_parsed == 'T'] # 109 rows\n",
    "\n",
    "# Create span columns. Split by space.\n",
    "split_ann = sampled_ann.annotation.str.split(' ', expand = True)\n",
    "\n",
    "# Rename Columns\n",
    "split_ann.columns = ['Entities', 'first_char', 'last_char'] # It's already sorted by first_char ascending\n",
    "\n",
    "# Create new columns with each annotation's text.\n",
    "split_ann['text'] = sampled_ann.loc[sampled_ann.id_parsed == 'T', 'text']\n",
    "\n",
    "split_ann.first_char = split_ann.first_char.astype(int)\n",
    "split_ann.last_char = split_ann.last_char.astype(int)\n",
    "\n",
    "split_ann.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Text and Merge with Annotation File\n",
    "\n",
    "<br>\n",
    "\n",
    "Approach\n",
    "\n",
    "- The base strategy is to find the starting position of each new line and join it with the the first_char column in the annotation table. \n",
    "\n",
    "<br>\n",
    "\n",
    "Roadblock\n",
    "\n",
    "- The problem with this approach is that the text file's lines do not match with the annotation file's span. This is due that the span is for the tag, not necessarily the first word of the tweet is going to be tagged. The same is for the last tag's span vs the end of the string. \n",
    "\n",
    "<br>\n",
    "\n",
    "To do:\n",
    "\n",
    "- I am thinking on using fuzzy string matching to match `split_ann` text column with the original text data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:04:20.053155Z",
     "start_time": "2020-07-26T02:04:20.026015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import Regex Module\n",
    "import re\n",
    "\n",
    "# Set max column width to none in order to read all the tweet's text. \n",
    "pd.options.display.max_colwidth = None\n",
    "\n",
    "# Read text\n",
    "text = pd.read_csv(txt, header=None )\n",
    "\n",
    "## Deprecated path\n",
    "# ../brat-v1.3_Crunchy_Frog/data/first-iter/balanced_dataset_brat.txt\n",
    "\n",
    "# Rename column to tweets\n",
    "text.columns = ['tweets']\n",
    "\n",
    "# Convert tweets to string\n",
    "text_string = text.tweets.to_string( header = False)\n",
    "\n",
    "# Remove extra spaces and replace them with just 1 space.\n",
    "text_string = re.sub(' +', ' ', text_string)\n",
    "\n",
    "#last_char = [pos for pos, char in enumerate(text_string) if char == '\\n']\n",
    "\n",
    "# last_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt to solving the roadblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:04:29.808388Z",
     "start_time": "2020-07-26T02:04:29.786041Z"
    }
   },
   "outputs": [],
   "source": [
    "# read the txt as a string file\n",
    "with open(txt) as f:\n",
    "    # only replace the break lines\n",
    "    REPLACE_br = lambda s: s.replace(\"\\n\",\"\\n\")\n",
    "    lines = map( REPLACE_br, f.readlines() )\n",
    "    \n",
    "    # save number line, length of the text and text without break lines: /n\n",
    "    # assuming one line corresponds to a single tweet\n",
    "    tuple_tweets = [(len(l), l) for l in lines if len(l) > 0]\n",
    "\n",
    "    start, end, text_ = list(), list(), list()\n",
    "    new_start = 0\n",
    "    for ttw in tuple_tweets:\n",
    "        # adds the length of the tweet\n",
    "        start.append(new_start)\n",
    "        # finds the location of the last character of the tweet\n",
    "        end.append(new_start + ttw[0] -1 )\n",
    "        text_.append(ttw[1])\n",
    "        \n",
    "        # gets the starting position of the next tweet\n",
    "        new_start = new_start + ttw[0]\n",
    "\n",
    "    text_df = pd.DataFrame({\n",
    "            \"first_char\": start,\n",
    "            \"last_char\": end,\n",
    "            \"text\": text_\n",
    "            })\n",
    "# Text - Fix de Min!!\n",
    "text_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:04:33.285153Z",
     "start_time": "2020-07-26T02:04:33.255128Z"
    }
   },
   "outputs": [],
   "source": [
    "# Annotations DF to test \n",
    "\n",
    "split_ann = split_ann.sort_values('first_char').copy()\n",
    "split_ann.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt - Merging Text and Labels \n",
    "\n",
    "<br>\n",
    "\n",
    "This approach doesn't take into consideration tags that were made in the middle of the tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:04:36.514945Z",
     "start_time": "2020-07-26T02:04:34.312949Z"
    }
   },
   "outputs": [],
   "source": [
    "## This didn't work\n",
    "# pd.merge(df, split_ann, on = ['last_char'] ).info()\n",
    "\n",
    "\n",
    "text = [] # Load the matched text into a list \n",
    "ann = [] # Load the matched labels into a list\n",
    "\n",
    "# For each text_df rows and for each split_ann rows\n",
    "for index_txt, row_txt in text_df.iterrows():\n",
    "    \n",
    "    for index_ann, row_ann in split_ann.iterrows():\n",
    "    \n",
    "        # If first char of annotations and text match \n",
    "            # OR\n",
    "        # If last_char of annotations matches text \n",
    "        if row_ann[1] == row_txt[0] or row_ann[2] == row_txt[1] :\n",
    "            ann.append(row_ann[0])\n",
    "            text.append(row_txt[2])\n",
    "        else:\n",
    "            next\n",
    "\n",
    "\n",
    "txt_ann_df = pd.DataFrame({\n",
    "                    'text':text, \n",
    "                    'ann':ann\n",
    "                    })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:04:36.532563Z",
     "start_time": "2020-07-26T02:04:36.517237Z"
    }
   },
   "outputs": [],
   "source": [
    "### Merging the txt with annotations\n",
    "\n",
    "# Create Final Dataframe\n",
    "txt_ann_df_concat = pd.concat([txt_ann_df, # Final Dataframe\n",
    "           pd.get_dummies(txt_ann_df.ann)] # Getting dummies of all annotations and append both Dfs\n",
    "          , axis = 1)\n",
    "\n",
    "# Group by each tweet's text. The tweets are duplicated from last step \n",
    "    # (if they matched both in first_char and last_char)\n",
    "\n",
    "    ## Tags are missing in this approach\n",
    "# txt_ann_df_concat.drop(['ann'], axis = 1).groupby('text', as_index = False).sum() #drop annotation column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second Attempt - Merging Text and Labels\n",
    "\n",
    "This approach takes into consideration the complete span of the text. \n",
    "\n",
    "It subsets all the tags that are within the beginning of the tweet and the end of the tweet. A new column \"multi_id\" is created in order to match both dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:04:39.068921Z",
     "start_time": "2020-07-26T02:04:38.859786Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize column multi_id\n",
    "split_ann['multi_id'] = 0\n",
    "\n",
    "# For each row of original text \n",
    "for i in range(text_df.shape[0]):\n",
    "    \n",
    "            # Match first character position of the annotation greater or equal to the first_char of the tweet\n",
    "            # Match last character position of the annotation less than the last character of the complete tweet\n",
    "            # This will subset the annotations on the complete span of the tweet.\n",
    "    idx = split_ann[(split_ann.first_char >= text_df.loc[i,'first_char']) &\\\n",
    "              (split_ann.first_char < text_df.loc[i,'last_char'])].index   \n",
    "\n",
    "#     Create common value for each dataframe. \n",
    "    split_ann.loc[idx, 'multi_id'] = i \n",
    "    text_df.loc[i, 'multi_id'] = i\n",
    "\n",
    "\n",
    "# Merge both dataframes and select the entities and the tweets' complete text\n",
    "merged = pd.merge(split_ann, text_df, on='multi_id').loc[:, ['Entities', 'text_y']]\n",
    "\n",
    "\n",
    "# Create dummy variables from entities (these are the tags)\n",
    "dummies = pd.get_dummies(merged.Entities)\n",
    "\n",
    "# Group by the complete text\n",
    "pd.concat([merged['text_y'], dummies], axis = 1).groupby('text_y').sum().reset_index().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper function implementation\n",
    "\n",
    "The output of this helper function is intended to serve as the input for the baseline machine learning model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-26T02:47:40.038768Z",
     "start_time": "2020-07-26T02:47:39.802463Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append root folder\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "from src.c4v.data.data_loader import BratDataLoader\n",
    "\n",
    "# Instantiate class with one dataset\n",
    "loaded_data = BratDataLoader(['../data/processed/brat/sampled_58_30'], binary=True)\n",
    "\n",
    "## Get parsed dataframe with text and one-hot encoded responses\n",
    "loaded_data.get_parsed_df()\n",
    "\n",
    "# Create training and test sets\n",
    "loaded_data.preprocess()\n",
    "\n",
    "## Print training and test sets\n",
    "print(\n",
    "f'''\n",
    "Training set shape:\n",
    "X_train: {loaded_data.X_train.shape}\n",
    "y_train: {loaded_data.y_train.shape}\n",
    "\n",
    "Testing set shape:\n",
    "X_test: {loaded_data.X_test.shape}\n",
    "y_test: {loaded_data.y_test.shape}\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "704px",
    "left": "1076px",
    "top": "293px",
    "width": "367px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "362px",
    "left": "1185px",
    "right": "20px",
    "top": "129px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
