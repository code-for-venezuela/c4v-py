{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You must include swifter and multiprocessing in your lock file. The lock file won't be merged to master\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import swifter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-implement",
   "metadata": {},
   "source": [
    "# Reading data and normalizing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"../../data/raw/ovsp_bdd_octubre.xlsx\")\n",
    "column_names_normalized = df.columns.str.strip().str.lower().str.replace(\" \", \"_\")\n",
    "df.columns = column_names_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-fitness",
   "metadata": {},
   "source": [
    "# Quick and Dirty EDA: Confirming that I'm subsetting the correct number of elpitazo news\n",
    "\n",
    "There are 37 duplicated `el pitazo` rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-agency",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confirming that I'm subsetting the correct number of elpitazo links\n",
    "    # Result: 2483 el pitazo links\n",
    "pitazo_mask = df.link_de_la_noticia.str.contains(\"https://elpitazo.net\",na=False)\n",
    "df_elpitazo = df[pitazo_mask]\n",
    "# df.link_de_la_noticia[pitazo_mask].str.split(\"/\", expand = True)[2].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brave-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicated values\n",
    "\n",
    "mask_duplicated = df_elpitazo.duplicated()\n",
    "# df_elpitazo[mask_duplicated].sort_values(\"link_de_la_noticia\") ## Uncomment to check duplicated rows\n",
    "\n",
    "mask_duplicated.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-driving",
   "metadata": {},
   "source": [
    "# Elpitazo webscraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-aurora",
   "metadata": {},
   "source": [
    "Common classes from `elpitazo`:\n",
    "\n",
    "- Titlo de noticia `tdb-title-text`\n",
    "- info general de la noticia `tdb-title-text`\n",
    "- Texto completo de la noticia class=\"tdb-block-inner td-fix-index\"\n",
    "                            id=\"bsf_rt_marker\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-pride",
   "metadata": {},
   "source": [
    "## Testing with different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-peeing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing different classes\n",
    "# The easiest way to extract the news content is to extract all p tags and then clean them.\n",
    "\n",
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "URL = \"https://elpitazo.net/los-llanos/vecinos-de-varias-comunidades-de-acarigua-tienen-siete-anos-sin-agua/\"\n",
    "page = requests.get(URL, headers=headers, timeout=20)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "soup.find_all(\"h1\", {\"class\": \"tdb-title-text\"}) # This snippet works well to find the news article title\n",
    "# # soup.find_all(\"div\", {\"class\": \"tdb-block-inner td-fix-index\"})\n",
    "# soup.find_all(\"p\", {\"class\": \"tdb-block-inner td-fix-index\"})\n",
    "\n",
    "# ls = []\n",
    "\n",
    "# tags_p = soup.find_all(\"p\")\n",
    "title = soup.get_element_text(\".tdb-title-text\", response) or \"\"\n",
    "date = soup.get_element_text(\".entry-date\", response) or \"\"\n",
    "author = soup.get_element_text(\".tdb-author-name\", response) or \"\"\n",
    "\n",
    "# body = self._get_body(response)\n",
    "\n",
    "# for tags in tags_p:\n",
    "    \n",
    "#     if tags.has_attr('class') and tags['class'][0] in ['contacto-datos', 'text-white', '__cf_email__']:\n",
    "#         continue\n",
    "        \n",
    "#     ls.append(tags.get_text())\n",
    "\n",
    "# ' '.join(ls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-frederick",
   "metadata": {},
   "source": [
    "### Webscraper first pass\n",
    "\n",
    "TODO: Use multiprocessing instead of pandas.apply. There are too many links to do it in a sequential fashion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def webscraper_elpitazo(url:str):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    page = requests.get(url, headers=headers, timeout=20)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    \n",
    "    # Title\n",
    "    title = soup.find(\"h1\", {\"class\": \"tdb-title-text\"}) or \"\"\n",
    "    if type(title) != str:\n",
    "        title = title.get_text()\n",
    "\n",
    "    # Date and time\n",
    "    date = soup.find(\"time\", {\"class\":\"entry-date\"}) or \"\"\n",
    "    if type(date) != str:\n",
    "        date = date.get_text()\n",
    "\n",
    "    # Author\n",
    "    author = soup.find(\"a\", {\"class\":\"tdb-author-name\"}) or \"\"\n",
    "    if type(author) != str:\n",
    "        author = author.get_text()\n",
    "\n",
    "    # Tags. E.g., Acarigua, falta de agua, portuguesa\n",
    "    etiquetas = soup.find(\"ul\", {\"class\":\"tdb-tags\"}) or \"\"\n",
    "    if type(etiquetas) != str:\n",
    "        etiquetas = \", \".join(list(etiquetas.stripped_strings))\n",
    "\n",
    "    # Category. E.g., los llanos\n",
    "    category = soup.find(\"a\", {\"class\":\"tdb-entry-category\"}) or \"\"\n",
    "    if type(category) != str:\n",
    "        category = category.get_text() or \"\"\n",
    "\n",
    "    # News Text\n",
    "    tags_p = soup.find_all(\"p\")\n",
    "\n",
    "    ls = [] # Temporary list\n",
    "    for tags in tags_p:\n",
    "\n",
    "        if tags.has_attr('class') and tags['class'][0] in ['contacto-datos', 'text-white', '__cf_email__']:\n",
    "            continue\n",
    "\n",
    "        ls.append(tags.get_text())\n",
    "\n",
    "    text = ' '.join(ls)\n",
    "    \n",
    "    webscraped_dic = {\n",
    "        \"url\":url,\n",
    "        \"title\":title,\n",
    "        \"date\":date,\n",
    "        \"author\":author,\n",
    "        \"tags\":etiquetas,\n",
    "        \"category\":category,\n",
    "        \"text\":text\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return webscraped_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-mailman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single URL Test\n",
    "URL = \"https://elpitazo.net/los-llanos/el-gas-domestico-en-acarigua-araure-cuesta-entre-10-y-20-dolares/\"\n",
    "webscraper_elpitazo(url=URL)\n",
    "\n",
    "elpitazo_text = df_elpitazo.head().link_de_la_noticia.apply(webscraper_elpitazo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-legislation",
   "metadata": {},
   "source": [
    "## Multiprocessing implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-scout",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel processing. \n",
    "# First try: Didn't work\n",
    "# def parallelize_dataframe(df, func):\n",
    "#     num_processes = mp.cpu_count()\n",
    "    \n",
    "#     df_split = np.array_split(df, num_processes)\n",
    "    \n",
    "#     with mp.Pool(num_processes) as p:\n",
    "#         df = pd.concat(p.map(func, df_split))\n",
    "#     return df\n",
    "\n",
    "# parallelize_dataframe(df_elpitazo.link_de_la_noticia.head(),webscraper_elpitazo )\n",
    "\n",
    "# Second try with swifter library: THIS WORKED, however there must be a faster solution\n",
    "\n",
    "# Swifter uses a normal pandas apply. Main constraint: It takes quite a bit of time (around 30 min)\n",
    "elpitazo_textfull = df_elpitazo.link_de_la_noticia.swifter.apply(webscraper_elpitazo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "elpitazo_webscraped_df = pd.DataFrame.from_dict(list(elpitazo_textfull))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sized-lewis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save webscraped text\n",
    "\n",
    "elpitazo_textfull.to_csv(\"../../data/interim/webscraping/json_links_elpitazo_04242021.csv\") # I'm saving the json file just in case\n",
    "elpitazo_webscraped_df.to_csv(\"../../data/interim/webscraping/links_elpitazo_04242021.csv\")\n",
    "\n",
    "# Merge webscraped data with tagged dataframe\n",
    "    # Note: In the merge, duplicated values are created. \n",
    "    # This is due that I'm joining by URL. In the original tagged data, urls are not unique. \n",
    "    # The urls are not unique because one url may have different event types (falta de servicio, protesta)\n",
    "df_elpitazo_webscrape_join_tags = pd.merge(df_elpitazo,elpitazo_webscraped_df, left_on= \"link_de_la_noticia\", \n",
    "                                right_on= \"url\", how = \"left\", suffixes = (\"_original\", \"_scraped\"))\n",
    "\n",
    "# Drop duplicates\n",
    "df_elpitazo_webscrape_join_tags_nodups = df_elpitazo_webscrape_join_tags.drop_duplicates()\n",
    "f\"Total amount of duplicates after join: {df_elpitazo_webscrape_join_tags.shape[0] - df_elpitazo_webscrape_join_tags_nodups.shape[0]} records\"\n",
    "\n",
    "df_elpitazo_webscrape_join_tags_nodups.to_csv(\"../../data/processed/webscraping/elpitazo_positivelabels_devdataset.csv\",index = False)\n",
    "\n",
    "# # Save link noticia and full text to check the quality of the web scraper\n",
    "# df_elpitazo_fulltext[[\"link_de_la_noticia\",\"link_de_la_noticia_fulltext\"]].to_csv(\"../../data/interim/webscraping/fulltext_links_elpitazo_03132021.csv\")\n",
    "\n",
    "# # Remove columns that were empty (this happened because I read the data from excel)\n",
    "# unnamed_cols = df_elpitazo_fulltext.columns[df_elpitazo_fulltext.columns.str.contains(\"unnamed\")]\n",
    "# df_elpitazo_fulltext = df_elpitazo_fulltext.drop(unnamed_cols, axis = 1)\n",
    "# df_elpitazo_fulltext.to_csv(\"../../data/processed/webscraping/ovsp_bdd_elpitazotext.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
