{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "byte_pair_encoding_w_sentencepiece",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jurqKXarU52c",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**First half of the notebook is implementing BPE from scratch**.  This example starts with a small working vocab.  This section is just for educational purposes.\n",
        "\n",
        "\n",
        "\n",
        "**Second half of the notebook is using the sentencepiece package**.  This example starts with raw text from a .txt file that we download.  We can substitute this .txt file for our own.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj2Zv2rA-LjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "\n",
        "import re\n",
        "import time\n",
        "import numpy as np\n",
        "from operator import itemgetter\n",
        "from typing import Dict, Tuple, List, Set\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LQy2GQ4_V8v",
        "colab_type": "text"
      },
      "source": [
        "Credit for code:  http://ethen8181.github.io/machine-learning/deep_learning/subword/bpe.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbKEn-w-_VsM",
        "colab_type": "text"
      },
      "source": [
        "Start by initializing the vocabulary with character vocabulary plus a special end of word symbol\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tofv5vff8EZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "3c79dff6-bdb4-42d9-cb6b-8190d7966678"
      },
      "source": [
        "# Credit: http://ethen8181.github.io/machine-learning/deep_learning/subword/bpe.html\n",
        "\n",
        "\n",
        "# assuming we've extracted from our raw text and this is the character\n",
        "# vocabulary that we've ended up with, along with their frequency\n",
        "vocab = {\n",
        "    'l o w </w>': 5,\n",
        "    'l o w e r </w>': 2,\n",
        "    'n e w e s t </w>': 6,\n",
        "    'w i d e s t </w>': 3,\n",
        "    'h a p p i e r </w>': 2\n",
        "}\n",
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'h a p p i e r </w>': 2,\n",
              " 'l o w </w>': 5,\n",
              " 'l o w e r </w>': 2,\n",
              " 'n e w e s t </w>': 6,\n",
              " 'w i d e s t </w>': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ter15dQ_o9P",
        "colab_type": "text"
      },
      "source": [
        "We then cound the frequence of each consecutive character pair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKp4Jcf9-Kp4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pair_stats(vocab: Dict[str, int]) -> Dict[Tuple[str, str], int]:\n",
        "    \"\"\"Get counts of pairs of consecutive symbols.\"\"\"\n",
        "\n",
        "    pairs = {}\n",
        "    for word, frequency in vocab.items():\n",
        "        symbols = word.split()\n",
        "\n",
        "        # count occurrences of pairs\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pair = (symbols[i], symbols[i + 1])\n",
        "            current_frequency = pairs.get(pair, 0)\n",
        "            pairs[pair] = current_frequency + frequency\n",
        "\n",
        "    return pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC3qWVc1-6uN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "43fe8598-d1ed-406c-ce46-2ddd22526421"
      },
      "source": [
        "pair_stats = get_pair_stats(vocab)\n",
        "pair_stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('a', 'p'): 2,\n",
              " ('d', 'e'): 3,\n",
              " ('e', 'r'): 4,\n",
              " ('e', 's'): 9,\n",
              " ('e', 'w'): 6,\n",
              " ('h', 'a'): 2,\n",
              " ('i', 'd'): 3,\n",
              " ('i', 'e'): 2,\n",
              " ('l', 'o'): 7,\n",
              " ('n', 'e'): 6,\n",
              " ('o', 'w'): 7,\n",
              " ('p', 'i'): 2,\n",
              " ('p', 'p'): 2,\n",
              " ('r', '</w>'): 4,\n",
              " ('s', 't'): 9,\n",
              " ('t', '</w>'): 9,\n",
              " ('w', '</w>'): 5,\n",
              " ('w', 'e'): 8,\n",
              " ('w', 'i'): 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOomy9RK_7Gw",
        "colab_type": "text"
      },
      "source": [
        "We find the most frequent, and merge them together into a new token whenever we encounter them in the vocabulary.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r66K1YOp-7Il",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def merge_vocab(best_pair: Tuple[str, str], vocab_in: Dict[str, int]) -> Dict[str, int]:\n",
        "    \"\"\"Step 3. Merge all occurrences of the most frequent pair\"\"\"\n",
        "\n",
        "    vocab_out = {}\n",
        "\n",
        "    # re.escape\n",
        "    # ensures the characters of our input pair will be handled as is and\n",
        "    # not get mistreated as special characters in the regular expression.\n",
        "    pattern = re.escape(' '.join(best_pair))\n",
        "    replacement = ''.join(best_pair)\n",
        "\n",
        "    for word_in in vocab_in:\n",
        "        # replace most frequent pair in all vocabulary\n",
        "        word_out = re.sub(pattern, replacement, word_in)\n",
        "        vocab_out[word_out] = vocab_in[word_in]\n",
        "\n",
        "    return vocab_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBnYKqqRAfBP",
        "colab_type": "text"
      },
      "source": [
        "In this particular round, the e,s consecutive pair was identified as the most frequent pair, then in the new vocabulary, all the e,s was merged together into a single token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5yWlUGS-7M9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "58d54d5b-2da8-446a-80c7-72e3d71a90fb"
      },
      "source": [
        "best_pair = max(pair_stats, key=pair_stats.get)\n",
        "print(best_pair)\n",
        "\n",
        "new_vocab = merge_vocab(best_pair, vocab)\n",
        "new_vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('e', 's')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'h a p p i e r </w>': 2,\n",
              " 'l o w </w>': 5,\n",
              " 'l o w e r </w>': 2,\n",
              " 'n e w es t </w>': 6,\n",
              " 'w i d es t </w>': 3}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsaQiIXFAzRN",
        "colab_type": "text"
      },
      "source": [
        "This was only 1 iteration of the merging process, we can iteratively perform this merging step until we reach the number of merges or the number of tokens we would like to have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0hzEjCc-7Rb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "outputId": "fd2e9aac-5c38-48b5-b164-cd83d5272ba2"
      },
      "source": [
        "vocab = {\n",
        "    'l o w </w>': 5,\n",
        "    'l o w e r </w>': 2,\n",
        "    'n e w e s t </w>': 6,\n",
        "    'w i d e s t </w>': 3,\n",
        "    'h a p p i e r </w>': 2\n",
        "}\n",
        "\n",
        "# we store the best pair during each iteration for encoding new vocabulary, more on this later\n",
        "bpe_codes = {}\n",
        "num_merges = 10  # hyperparameter\n",
        "for i in range(num_merges):\n",
        "    print('\\niteration', i)\n",
        "    pair_stats = get_pair_stats(vocab)\n",
        "    if not pair_stats:\n",
        "        break\n",
        "\n",
        "    best_pair = max(pair_stats, key=pair_stats.get)\n",
        "    bpe_codes[best_pair] = i\n",
        "\n",
        "    print('vocabulary: ', vocab)\n",
        "    print('best pair:', best_pair)\n",
        "    vocab = merge_vocab(best_pair, vocab)\n",
        "\n",
        "print('\\nfinal vocabulary: ', vocab)\n",
        "print('\\nbyte pair encoding: ', bpe_codes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "iteration 0\n",
            "vocabulary:  {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w e s t </w>': 6, 'w i d e s t </w>': 3, 'h a p p i e r </w>': 2}\n",
            "best pair: ('e', 's')\n",
            "\n",
            "iteration 1\n",
            "vocabulary:  {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w es t </w>': 6, 'w i d es t </w>': 3, 'h a p p i e r </w>': 2}\n",
            "best pair: ('es', 't')\n",
            "\n",
            "iteration 2\n",
            "vocabulary:  {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est </w>': 6, 'w i d est </w>': 3, 'h a p p i e r </w>': 2}\n",
            "best pair: ('est', '</w>')\n",
            "\n",
            "iteration 3\n",
            "vocabulary:  {'l o w </w>': 5, 'l o w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
            "best pair: ('l', 'o')\n",
            "\n",
            "iteration 4\n",
            "vocabulary:  {'lo w </w>': 5, 'lo w e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
            "best pair: ('lo', 'w')\n",
            "\n",
            "iteration 5\n",
            "vocabulary:  {'low </w>': 5, 'low e r </w>': 2, 'n e w est</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
            "best pair: ('n', 'e')\n",
            "\n",
            "iteration 6\n",
            "vocabulary:  {'low </w>': 5, 'low e r </w>': 2, 'ne w est</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
            "best pair: ('ne', 'w')\n",
            "\n",
            "iteration 7\n",
            "vocabulary:  {'low </w>': 5, 'low e r </w>': 2, 'new est</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
            "best pair: ('new', 'est</w>')\n",
            "\n",
            "iteration 8\n",
            "vocabulary:  {'low </w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
            "best pair: ('low', '</w>')\n",
            "\n",
            "iteration 9\n",
            "vocabulary:  {'low</w>': 5, 'low e r </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3, 'h a p p i e r </w>': 2}\n",
            "best pair: ('e', 'r')\n",
            "\n",
            "final vocabulary:  {'low</w>': 5, 'low er </w>': 2, 'newest</w>': 6, 'w i d est</w>': 3, 'h a p p i er </w>': 2}\n",
            "\n",
            "byte pair encoding:  {('e', 's'): 0, ('es', 't'): 1, ('est', '</w>'): 2, ('l', 'o'): 3, ('lo', 'w'): 4, ('n', 'e'): 5, ('ne', 'w'): 6, ('new', 'est</w>'): 7, ('low', '</w>'): 8, ('e', 'r'): 9}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5kgSWZ9Bq32",
        "colab_type": "text"
      },
      "source": [
        "# Applying Encodings\n",
        "\n",
        "Now that we've seen the process of \"learning\" the byte pair encodings.  We now turn our attention to the process of \"applying\" them to a new vocabulary.\n",
        "\n",
        "While possible:\n",
        "\n",
        "\n",
        "*   Get all the bigram symbols for the word that we wish to encode.\n",
        "*   Find the symbol pair in our byte pair codes that appeared first among the symbols that were merged.\n",
        "*   Apply the merge on the word\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH-RORoy-7Uj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c073935-5b00-4260-d966-ed8d3799d561"
      },
      "source": [
        "# first convert an input word to the list of character format\n",
        "original_word = 'lowest'\n",
        "word = list(original_word)\n",
        "word.append('</w>')\n",
        "word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['l', 'o', 'w', 'e', 's', 't', '</w>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAZhzzkn-7Xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pairs(word: List[str]) -> Set[Tuple[str, str]]:\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "\n",
        "    return pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWQrPcoS-7bm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0982143-5abf-4242-f671-07a2c0b4ed78"
      },
      "source": [
        "# gets the set of possible bigram symbol\n",
        "pairs = get_pairs(word)\n",
        "pairs"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('e', 's'), ('l', 'o'), ('o', 'w'), ('s', 't'), ('t', '</w>'), ('w', 'e')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xlz5ROEi-7e8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49f9fd50-f55b-4176-e88a-b08b00b3b577"
      },
      "source": [
        "# attempt to find it in the byte pair codes\n",
        "bpe_codes_pairs = [(pair, bpe_codes[pair]) for pair in pairs if pair in bpe_codes]\n",
        "pair_to_merge = min(bpe_codes_pairs, key=itemgetter(1))[0]\n",
        "pair_to_merge"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('e', 's')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAJIKwUJ-7hx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_new_word(word: List[str], pair_to_merge: Tuple[str, str]) -> List[str]:\n",
        "    first, second = pair_to_merge\n",
        "    new_word = []\n",
        "    i = 0\n",
        "    while i < len(word):\n",
        "        try:\n",
        "            j = word.index(first, i)\n",
        "            new_word.extend(word[i:j])\n",
        "            i = j\n",
        "        except ValueError:\n",
        "            new_word.extend(word[i:])\n",
        "            break\n",
        "\n",
        "        if i < len(word) - 1 and word[i + 1] == second:\n",
        "            new_word.append(first + second)\n",
        "            i += 2\n",
        "        else:\n",
        "            new_word.append(first)\n",
        "            i += 1\n",
        "\n",
        "    return new_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwp6CqmOD3h5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1eecdc15-50c1-47ba-d4b4-a56860a186a0"
      },
      "source": [
        "# stitch together the new word\n",
        "new_word = create_new_word(word, pair_to_merge)\n",
        "new_word"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['l', 'o', 'w', 'es', 't', '</w>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee2doB23EAMA",
        "colab_type": "text"
      },
      "source": [
        "The previous couple of code chunks shows one iteration of applying the byte pair codes to encode a new word.  The next one puts everything together into a single function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w82FyRXoD3ga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(original_word: str, bpe_codes: Dict[Tuple[str, str], int]) -> List[str]:\n",
        "    if len(original_word) == 1:\n",
        "        return original_word\n",
        "\n",
        "    word = list(original_word)\n",
        "    word.append('</w>')\n",
        "\n",
        "    while True:\n",
        "        pairs = get_pairs(word)\n",
        "        bpe_codes_pairs = [(pair, bpe_codes[pair]) for pair in pairs if pair in bpe_codes]\n",
        "        if not bpe_codes_pairs:\n",
        "            break\n",
        "\n",
        "        pair_to_merge = min(bpe_codes_pairs, key=itemgetter(1))[0]\n",
        "        word = create_new_word(word, pair_to_merge)\n",
        "\n",
        "    return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1VovMTbD3eR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7092773b-5655-4c4f-c174-79865b1314ba"
      },
      "source": [
        "original_word = 'lowest'\n",
        "encode(original_word, bpe_codes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['low', 'est</w>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtq56FTMD3bc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "bdf4be4a-244d-4c61-bb1a-cd4fdc97baa8"
      },
      "source": [
        "bpe_codes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('e', 'r'): 9,\n",
              " ('e', 's'): 0,\n",
              " ('es', 't'): 1,\n",
              " ('est', '</w>'): 2,\n",
              " ('l', 'o'): 3,\n",
              " ('lo', 'w'): 4,\n",
              " ('low', '</w>'): 8,\n",
              " ('n', 'e'): 5,\n",
              " ('ne', 'w'): 6,\n",
              " ('new', 'est</w>'): 7}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tnJOHlNEZg-",
        "colab_type": "text"
      },
      "source": [
        "Judging from the result, we can see that even though the word lowest did not appear in our \"training\" data, but because \"low\" and and ending \"est\" are both byte pair codes that were learned, the word got encoded into low est."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5Pdc9zqGN-L",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Sentencepiece\n",
        "\n",
        "https://github.com/google/sentencepiece\n",
        "\n",
        "Upon seeing an educational implementation, we will give a more efficient implementation, **Sentencepiece**, a swing.  Read https://github.com/google/sentencepiece#overview  for a high-level introduction to the package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVdzHa4aD3W2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "b93b06ce-c428-41ff-ec27-9b1220273f89"
      },
      "source": [
        "# download some sample text for demonstration\n",
        "!wget https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-10 22:21:35--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt.6’\n",
            "\n",
            "\rbotchan.txt.6         0%[                    ]       0  --.-KB/s               \rbotchan.txt.6       100%[===================>] 272.25K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-08-10 22:21:35 (4.55 MB/s) - ‘botchan.txt.6’ saved [278779/278779]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_au-7_UPKRpU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d906021-c82c-4c65-90e6-84e35e36988f"
      },
      "source": [
        "!pip install sentencepiece\n",
        "\n",
        "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Np1ibB5uLgtV",
        "colab_type": "text"
      },
      "source": [
        "To train the subword unit, we call the SentencePieceTrainer.train method by passing in our parameters. I've specified some of the commonly used parameters in the example below. As for what are the available options, I find it helpful to just search in the source code of the parameter parser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atJPwcDKKR-c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c63424af-617b-4397-bf5b-b632f932b49f"
      },
      "source": [
        "from sentencepiece import SentencePieceTrainer, SentencePieceProcessor\n",
        "\n",
        "input_file = 'botchan.txt'\n",
        "max_num_words = 10000\n",
        "model_type = 'bpe'\n",
        "model_prefix = 'sentencepiece'\n",
        "pad_id = 0\n",
        "unk_id = 1\n",
        "bos_id = 2\n",
        "eos_id = 3\n",
        "\n",
        "sentencepiece_params = ' '.join([\n",
        "    '--input={}'.format(input_file),\n",
        "    '--model_type={}'.format(model_type),\n",
        "    '--model_prefix={}'.format(model_prefix),\n",
        "    '--vocab_size={}'.format(max_num_words),\n",
        "    '--pad_id={}'.format(pad_id),\n",
        "    '--unk_id={}'.format(unk_id),\n",
        "    '--bos_id={}'.format(bos_id),\n",
        "    '--eos_id={}'.format(eos_id)\n",
        "])\n",
        "print(sentencepiece_params)\n",
        "SentencePieceTrainer.train(sentencepiece_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--input=botchan.txt --model_type=bpe --model_prefix=sentencepiece --vocab_size=10000 --pad_id=0 --unk_id=1 --bos_id=2 --eos_id=3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4Nc8-rCK21_",
        "colab_type": "text"
      },
      "source": [
        "Some comments regarding the parameters:\n",
        "\n",
        "*  **input** expects a .txt file on disk. Hence, if we have a python variable, e.g. a list of sentences that we wish to learn the subword unit using sentencepiece, we would have to make an additional step to write it to a file on disk.\n",
        "\n",
        "*  **model_type** can take in bpe, unigram, char, word, allowing us to experiment with different tokenization schemes. The **unigram** method was not introduced in this notebook.\n",
        "\n",
        "*  **pad_id**, specifying these ids can be important with we're using sentencepiece in conjunction with other libraries, e.g. 1 library may have already by default preserved the token id 0 for padding characters, in that case, we can explicitly specifying that padding id to sentencepiece to be consistent.\n",
        "\n",
        "Upon training the model, we can load it, the model resides in **model_prefix**.model, where **model_prefix** is a parameter that we also get to specify."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIz6U4wsKSKG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d8077f56-56b1-4727-e4a0-716cca5dea29"
      },
      "source": [
        "sp = SentencePieceProcessor()\n",
        "sp.load(\"{}.model\".format(model_prefix))\n",
        "print('Found %s unique tokens.' % sp.get_piece_size())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 10000 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhi4FoYuMj8A",
        "colab_type": "text"
      },
      "source": [
        "Showcasing some common operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cS8iFr4pKSUc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "65b55c85-bc0b-4d40-efee-3edbca104b63"
      },
      "source": [
        "# encode: text => id\n",
        "# given a new text, we can convert it to subword units\n",
        "original = 'This is a test'\n",
        "encoded_pieces = sp.encode_as_pieces(original)\n",
        "print(encoded_pieces)\n",
        "\n",
        "# or convert it to numeric id for downstream modeling\n",
        "encoded_ids = sp.encode_as_ids(original)\n",
        "print(encoded_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁This', '▁is', '▁a', '▁t', 'est']\n",
            "[475, 98, 6, 4, 264]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFmzd3cSKScV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "417e3838-89c9-4ef6-b1ec-f5fc4fc93441"
      },
      "source": [
        "# decode: piece/id => text\n",
        "# we can convert the subword units back to the original text\n",
        "decoded_pieces = sp.decode_pieces(encoded_pieces)\n",
        "print(decoded_pieces)\n",
        "\n",
        "# we can convert the numeric id back to the original text\n",
        "decoded_ids = sp.decode_ids(encoded_ids)\n",
        "print(decoded_ids)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a test\n",
            "This is a test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoMrgngeKSki",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d872bebc-46e6-4413-a282-eebdd00bce22"
      },
      "source": [
        "# id <=> piece conversion\n",
        "original = '▁This'\n",
        "\n",
        "# finding the numeric id of the particular subword unit\n",
        "piece_id = sp.piece_to_id('▁This')\n",
        "print(piece_id)\n",
        "\n",
        "# obtaining the subword unit of a particular numeric id\n",
        "print(sp.id_to_piece(piece_id))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "475\n",
            "▁This\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icaRpgqYQVAQ",
        "colab_type": "text"
      },
      "source": [
        "# Basic end-to-end example\n",
        "\n",
        "credit:  https://nbviewer.jupyter.org/github/google/sentencepiece/blob/master/python/sentencepiece_python_module_example.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aHjeCbGKSzS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "91539e4f-d20a-446d-d109-0f19fcf6fc1c"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# train sentencepiece model from `botchan.txt` and makes `m.model` and `m.vocab`\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\n",
        "spm.SentencePieceTrainer.train('--input=botchan.txt --model_prefix=m --vocab_size=1000 --model_type=bpe')\n",
        "\n",
        "# makes segmenter instance and loads the model file (m.model)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# encode: text => id\n",
        "print(sp.encode_as_pieces('This is a test'))\n",
        "print(sp.encode_as_ids('This is a test'))\n",
        "\n",
        "# decode: id => text\n",
        "print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n",
        "print(sp.decode_ids([209, 31, 9, 375, 586]))\n",
        "\n",
        "print('*** BPE ***')\n",
        "print(sp.encode_as_pieces('thisisatesthelloworld'))\n",
        "print(sp.nbest_encode_as_pieces('hello world', 5))  # returns an empty list."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['▁This', '▁is', '▁a', '▁t', 'est']\n",
            "[474, 97, 5, 3, 263]\n",
            "This is a test\n",
            "areas theel pers\n",
            "*** BPE ***\n",
            "['▁this', 'is', 'at', 'est', 'he', 'llow', 'or', 'ld']\n",
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNh904Prfflp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTICwAyZff9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTXPy26YfgQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DUXXrcxfgMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAh-yTjVfgHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqunOvvlXl1z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3e1cd49-93cc-438d-ceda-eb987f18c424"
      },
      "source": [
        "!pip install tf_sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf_sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.90)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_MMFwLTYPeJ",
        "colab_type": "text"
      },
      "source": [
        "# SentencePiece TensorFlow module\n",
        "\n",
        "https://github.com/google/sentencepiece/blob/master/tensorflow/README.md"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntBJ4VI3bGb6",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6iXcA4yXmFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "outputId": "e33abc06-62b8-427d-db8d-2b9061fe7e57"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf_sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.90)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "--2020-08-10 22:21:51--  https://raw.githubusercontent.com/google/sentencepiece/master/data/botchan.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 278779 (272K) [text/plain]\n",
            "Saving to: ‘botchan.txt.7’\n",
            "\n",
            "botchan.txt.7       100%[===================>] 272.25K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2020-08-10 22:21:51 (4.73 MB/s) - ‘botchan.txt.7’ saved [278779/278779]\n",
            "\n",
            "Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.12.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.30.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (49.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iG3CJJ-XmJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}