{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1.0_diego_huggingface_poc.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nlYaraG_mds"
      },
      "source": [
        "# Transformers POC\n",
        "\n",
        "\n",
        "The objective of this notebook is to showcase a complete fine tuning of a custom dataset with Transformers.\n",
        "\n",
        "The dataset has been adapted to a binary classification problem.\n",
        "\n",
        "NOTE: Run this notebook in Google Colab and select a GPU runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP1ZgePu4Nyp"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyh883xQ4P0C"
      },
      "source": [
        "from google.colab import drive\n",
        "# drive.mount('/gdrive/My Drive/poc_transformers')\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRYVeUQT4YVR"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import RobertaModel, RobertaTokenizer, Trainer, TrainingArguments, RobertaForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sft7ZyeP_OYJ"
      },
      "source": [
        "# Reading Data & Data Wrangling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q9-qpDJ4gF-"
      },
      "source": [
        "df_elpitazo_pscdd = pd.read_csv(\"/content/drive/MyDrive/sambil/datasets/elpitazo_positivelabels_devdataset.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DoIQFz849Kq"
      },
      "source": [
        "print(df_elpitazo_pscdd.tipo_de_evento.value_counts().to_markdown())\n",
        "df_elpitazo_pscdd[\"label\"] = (df_elpitazo_pscdd.tipo_de_evento == \"DENUNCIA FALTA DEL SERVICIO\").astype(int)\n",
        "df_elpitazo_pscdd = df_elpitazo_pscdd.convert_dtypes()\n",
        "df_denuncia_texto = df_elpitazo_pscdd[[\"label\",\"text\"]]\n",
        "df_denuncia_texto.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtTSJSzv64Ya"
      },
      "source": [
        "# Training\n",
        "\n",
        "# Results\n",
        "|                         |   metrics_value |\n",
        "|:------------------------|----------------:|\n",
        "| eval_loss               |        1.24403  |\n",
        "| eval_accuracy           |        0.764583 |\n",
        "| eval_precision          |        0.755869 |\n",
        "| eval_recall             |        0.725225 |\n",
        "| eval_f1                 |        0.74023  |\n",
        "| eval_runtime            |       20.2013   |\n",
        "| eval_samples_per_second |       23.761    |\n",
        "| eval_steps_per_second   |        2.376    |\n",
        "| epoch                   |       50        |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b0Nttic5OB6"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import torch\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from transformers import RobertaModel, RobertaTokenizer # BertTokenizer, BertForSequenceClassification\n",
        "from transformers import EarlyStoppingCallback\n",
        "\n",
        "X = list(df_denuncia_texto[\"text\"])\n",
        "y = list(df_denuncia_texto[\"label\"])\n",
        "\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"mrm8488/RuPERTa-base\", num_labels=2)\n",
        "tokenizer = RobertaTokenizer.from_pretrained('mrm8488/RuPERTa-base')\n",
        "\n",
        "# Use GPU\n",
        "model.to(device)\n",
        "\n",
        "# Train Test Split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
        "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
        "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Create torch dataset\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels=None):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        if self.labels:\n",
        "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "train_dataset = Dataset(X_train_tokenized, y_train)\n",
        "val_dataset = Dataset(X_val_tokenized, y_val)\n",
        "\n",
        "def compute_metrics(p):\n",
        "    pred, labels = p\n",
        "    pred = np.argmax(pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
        "    recall = recall_score(y_true=labels, y_pred=pred)\n",
        "    precision = precision_score(y_true=labels, y_pred=pred)\n",
        "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5zE8dqR5Vot"
      },
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir= '/content/drive/MyDrive/sambil/poc_transformers/results',          # output directory\n",
        "    num_train_epochs=50,             # total # of training epochs\n",
        "    per_device_train_batch_size=10,  # batch size per device during training\n",
        "    per_device_eval_batch_size=10,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='/content/drive/MyDrive/sambil/poc_transformers/logs',            # directory for storing logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train pre-trained model\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"/content/drive/MyDrive/sambil/poc_transformers\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxb7oTLirfWJ"
      },
      "source": [
        "# Load Model\n",
        "loaded_model = RobertaForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/sambil/poc_transformers/ruperta_binary_denunciafaltaservicio\")\n",
        "\n",
        "# Define test trainer\n",
        "test_trainer = Trainer(loaded_model)\n",
        "\n",
        "# Make prediction\n",
        "raw_pred, _, _ = test_trainer.predict(val_dataset) # TODO: Use validation set instead of test set\n",
        "\n",
        "# Preprocess raw predictions\n",
        "y_pred = np.argmax(raw_pred, axis=1)\n",
        "\n",
        "\n",
        "## Evaluate Metrics\n",
        "metrics=test_trainer.evaluate(val_dataset)\n",
        "metrics_df = pd.DataFrame.from_dict(metrics, orient=\"index\",columns=[\"metrics_value\"])\n",
        "\n",
        "print(metrics_df.to_markdown())"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}