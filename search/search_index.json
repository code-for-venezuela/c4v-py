{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Microscope c4v-py \ud83d\udd2c Solving Venezuela pressing matters one commmit at a time c4v-py is a library used to address Venezuela's pressing issues using computer and data science. Installation Development Pending Installation Use pip to install the package: pip install c4v-py Usage The c4v-py package can be used either as a command line tool and as a library. Can you help us? Open a new issue in minutes! As a command line tool You can use the command line tool with the following command: c4v --help As a library Import the main interface, the microscope Manager to access a high level api to common operations: import c4v.microscope as ms # creates a manager object manager = ms.Manager() # crawl new urls from the internet d = manager.crawl_new_urls_for( [\"primicia\"], # Name of every available crawlers limit=10 # Maximum ammount of urls to crawl ) print(d) # A (possibly empty) list of urls as string print(len(d)) # a number <= 10 More about it here Contributing The following tools are used in this project: Poetry is used as package manager. Nox is used as automation tool, mainly for testing. Black is the mandatory formatter tool. PyEnv is recommended as a tool to handle multiple python versions in your machine. The library is intended to be compatible with python ~3.6.9, ~3.7.4 and ~3.8.2. But the primary version to support is ~3.8.2. The general structure of the project is trying to follow the recommendations in Cookiecutter Data Science . The main difference lies in the source code itself which is not constraint to data science code. Setup Install pyenv and select a version, ie: 3.8.2. Once installed run pyenv install 3.8.2 Install poetry in your system Clone this repo in a desired location git clone https://github.com/code-for-venezuela/c4v-py.git Navigate to the folder cd c4v-py Make sure your poetry picks up the right version of python by running pyenv local 3.8.2 , if 3.8.2 is your right version. Since our toml file is already created, we need to get all dependencies by running poetry install . This step might take a few minutes to complete. Install nox From c4v-py directory, on your terminal, run the command nox -s tests to make sure all the tests run. If you were able to follow every step with no error, you are ready to start contributing. Otherwise, open a new issue ! Roadmap [ ] Add CONTRIBUTING guidelines [ ] Add issue templates [ ] Document where to find things (datasets, more info, etc.) This might be done (in conjunction) with Github Projects. Managing tasks there might be a good idea. [ ] Add LICENSE [ ] Change the authors field in pyproject.toml [ ] Change the repository field in pyproject.toml [ ] Move the content below to a place near to the data in the data folder or use the reference folder. Check Cookiecutter Data Science for details. [ ] Understand what is in the following folders and decide what to do with them. [ ] brat-v1.3_Crunchy_Frog [ ] creating_models [x] data/data_to_annotate [ ] data_analysis [ ] Set symbolic links between brat-v1.3_Crunchy_Frog/data and data/data_to_annotate . data_sampler extracts to data/data_to_annotate . Files placed here are read by Brat. [ ] Download Brat - wget https://brat.nlplab.org/index.html [ ] untar brat - tar -xzvf brat-v1.3_Crunchy_Frog.tar.gz [ ] install brat - cd brat-v1.3_Crunchy_Frog && ./install.sh [ ] replace default annotation conf for current configuration - wget https://raw.githubusercontent.com/dieko95/c4v-py/master/brat-v1.3_Crunchy_Frog/annotation.conf -O annotation.conf [ ] replace default config.py for current configuration - wget https://raw.githubusercontent.com/dieko95/c4v-py/master/brat-v1.3_Crunchy_Frog/config.py -O config.py","title":"Home"},{"location":"#welcome-to-microscope-c4v-py","text":"Solving Venezuela pressing matters one commmit at a time c4v-py is a library used to address Venezuela's pressing issues using computer and data science. Installation Development Pending","title":"Welcome to Microscope c4v-py \ud83d\udd2c"},{"location":"#installation","text":"Use pip to install the package: pip install c4v-py","title":"Installation"},{"location":"#usage","text":"The c4v-py package can be used either as a command line tool and as a library. Can you help us? Open a new issue in minutes!","title":"Usage"},{"location":"#as-a-command-line-tool","text":"You can use the command line tool with the following command: c4v --help","title":"As a command line tool"},{"location":"#as-a-library","text":"Import the main interface, the microscope Manager to access a high level api to common operations: import c4v.microscope as ms # creates a manager object manager = ms.Manager() # crawl new urls from the internet d = manager.crawl_new_urls_for( [\"primicia\"], # Name of every available crawlers limit=10 # Maximum ammount of urls to crawl ) print(d) # A (possibly empty) list of urls as string print(len(d)) # a number <= 10 More about it here","title":"As a library"},{"location":"#contributing","text":"The following tools are used in this project: Poetry is used as package manager. Nox is used as automation tool, mainly for testing. Black is the mandatory formatter tool. PyEnv is recommended as a tool to handle multiple python versions in your machine. The library is intended to be compatible with python ~3.6.9, ~3.7.4 and ~3.8.2. But the primary version to support is ~3.8.2. The general structure of the project is trying to follow the recommendations in Cookiecutter Data Science . The main difference lies in the source code itself which is not constraint to data science code.","title":"Contributing"},{"location":"#setup","text":"Install pyenv and select a version, ie: 3.8.2. Once installed run pyenv install 3.8.2 Install poetry in your system Clone this repo in a desired location git clone https://github.com/code-for-venezuela/c4v-py.git Navigate to the folder cd c4v-py Make sure your poetry picks up the right version of python by running pyenv local 3.8.2 , if 3.8.2 is your right version. Since our toml file is already created, we need to get all dependencies by running poetry install . This step might take a few minutes to complete. Install nox From c4v-py directory, on your terminal, run the command nox -s tests to make sure all the tests run. If you were able to follow every step with no error, you are ready to start contributing. Otherwise, open a new issue !","title":"Setup"},{"location":"#roadmap","text":"[ ] Add CONTRIBUTING guidelines [ ] Add issue templates [ ] Document where to find things (datasets, more info, etc.) This might be done (in conjunction) with Github Projects. Managing tasks there might be a good idea. [ ] Add LICENSE [ ] Change the authors field in pyproject.toml [ ] Change the repository field in pyproject.toml [ ] Move the content below to a place near to the data in the data folder or use the reference folder. Check Cookiecutter Data Science for details. [ ] Understand what is in the following folders and decide what to do with them. [ ] brat-v1.3_Crunchy_Frog [ ] creating_models [x] data/data_to_annotate [ ] data_analysis [ ] Set symbolic links between brat-v1.3_Crunchy_Frog/data and data/data_to_annotate . data_sampler extracts to data/data_to_annotate . Files placed here are read by Brat. [ ] Download Brat - wget https://brat.nlplab.org/index.html [ ] untar brat - tar -xzvf brat-v1.3_Crunchy_Frog.tar.gz [ ] install brat - cd brat-v1.3_Crunchy_Frog && ./install.sh [ ] replace default annotation conf for current configuration - wget https://raw.githubusercontent.com/dieko95/c4v-py/master/brat-v1.3_Crunchy_Frog/annotation.conf -O annotation.conf [ ] replace default config.py for current configuration - wget https://raw.githubusercontent.com/dieko95/c4v-py/master/brat-v1.3_Crunchy_Frog/config.py -O config.py","title":"Roadmap"},{"location":"development/architecture/","text":"Architecture & components The Microscope library is compound by components that can be summarized as: Scraper : Will scrape data from known urls for specific websites, not every website might be scrapable , returning ScrapedData instances, this is the scheme for the data expected from a page Crawler : Will crawl new urls from specific sources, this data should be fed to the scraper at some point Persistency Manager : Will store data scraped by the scraper in some persistent storage, an SQLite-based manager is provided by default Classifier : Classifies a ScrapedData instance telling if it is a public service problem or not. Experiment : This class controls an experiment run, it's useful to manage logging and results for experiments. Also, it makes possible for every experiment to be ran in more or less the same way, making it easier to use for new comers. ExperimentFSManager : Simple class controlling how to experiment's filesystems are stored, enabling an unified filesystem for every experiment. You can implement a new object with the same interface if you want to provide an alternative method experiment's storage Warning The classifier should be more specific in the future, it should be able not only to differentiate between news talking about public services or not, but also the kind of problem itself Scraper The Scraper component is just a single function that receives a list of urls to scrape and manages to select the right scraper object for such url (based on its domain) or raise an error if it's not able to find any matching scraper . Example usage The next examples will show you how to use the scraper to scrape a list of urls, handle a possible non-valid url and filter out urls that may not be scrapable. Scraping multiple urls with the Manager object The easiest way to scrape is using the manager object as follows: import c4v.microscope as ms # Creates the default manager m = ms.Manager.from_default() urls = [ \"https://primicia.com.ve/mas/servicios/siete-trucos-caseros-para-limpiar-la-plancha-de-ropa/\", \"https://primicia.com.ve/guayana/ciudad/suenan-con-urbanismo-en-core-8/\" ] # Output may depend on your internet connection and page availability for result in m.scrape(urls): print(result.pretty_repr(max_content_len = 100)) Scraping a single url import c4v.microscope as ms m = ms.Manager.from_default() url = \"https://primicia.com.ve/mas/servicios/siete-trucos-caseros-para-limpiar-la-plancha-de-ropa/\" # Output may depend on your internet connection and page availability result = m.scrape(url) print(result.pretty_repr(max_content_len = 100)) Removing non-scrapable urls Here we can see how to separate scrapable urls from non-scrapable ones. It may be helpful to know which urls can be processed import c4v.microscope as ms m = ms.Manager.from_default() urls = [ \"https://primicia.com.ve\", \"https://elpitazo.net\", \"https://supernotscrapable.com\" ] assert m.split_non_scrapable(urls) == (urls[:2], urls[2:]) TODO add more useful examples Creation You can create a new scraper in order to support scraping for new sites. More details about this in \"creating a scraper\" Crawler TODO Creation You can create a new crawler in order to support exploring new urls for new sites. More details about this in \"creating a crawler\" Persistency Manager TODO Creation You can create a new Persistency Manager object in order to support new ways of storing data. More details about this in \"creating a persistency manager\" Experiment TODO ExperimentFSManager TODO","title":"Architecture"},{"location":"development/architecture/#architecture-components","text":"The Microscope library is compound by components that can be summarized as: Scraper : Will scrape data from known urls for specific websites, not every website might be scrapable , returning ScrapedData instances, this is the scheme for the data expected from a page Crawler : Will crawl new urls from specific sources, this data should be fed to the scraper at some point Persistency Manager : Will store data scraped by the scraper in some persistent storage, an SQLite-based manager is provided by default Classifier : Classifies a ScrapedData instance telling if it is a public service problem or not. Experiment : This class controls an experiment run, it's useful to manage logging and results for experiments. Also, it makes possible for every experiment to be ran in more or less the same way, making it easier to use for new comers. ExperimentFSManager : Simple class controlling how to experiment's filesystems are stored, enabling an unified filesystem for every experiment. You can implement a new object with the same interface if you want to provide an alternative method experiment's storage Warning The classifier should be more specific in the future, it should be able not only to differentiate between news talking about public services or not, but also the kind of problem itself","title":"Architecture &amp; components"},{"location":"development/architecture/#scraper","text":"The Scraper component is just a single function that receives a list of urls to scrape and manages to select the right scraper object for such url (based on its domain) or raise an error if it's not able to find any matching scraper .","title":"Scraper"},{"location":"development/architecture/#example-usage","text":"The next examples will show you how to use the scraper to scrape a list of urls, handle a possible non-valid url and filter out urls that may not be scrapable.","title":"Example usage"},{"location":"development/architecture/#scraping-multiple-urls-with-the-manager-object","text":"The easiest way to scrape is using the manager object as follows: import c4v.microscope as ms # Creates the default manager m = ms.Manager.from_default() urls = [ \"https://primicia.com.ve/mas/servicios/siete-trucos-caseros-para-limpiar-la-plancha-de-ropa/\", \"https://primicia.com.ve/guayana/ciudad/suenan-con-urbanismo-en-core-8/\" ] # Output may depend on your internet connection and page availability for result in m.scrape(urls): print(result.pretty_repr(max_content_len = 100))","title":"Scraping multiple urls with the Manager object"},{"location":"development/architecture/#scraping-a-single-url","text":"import c4v.microscope as ms m = ms.Manager.from_default() url = \"https://primicia.com.ve/mas/servicios/siete-trucos-caseros-para-limpiar-la-plancha-de-ropa/\" # Output may depend on your internet connection and page availability result = m.scrape(url) print(result.pretty_repr(max_content_len = 100))","title":"Scraping a single url"},{"location":"development/architecture/#removing-non-scrapable-urls","text":"Here we can see how to separate scrapable urls from non-scrapable ones. It may be helpful to know which urls can be processed import c4v.microscope as ms m = ms.Manager.from_default() urls = [ \"https://primicia.com.ve\", \"https://elpitazo.net\", \"https://supernotscrapable.com\" ] assert m.split_non_scrapable(urls) == (urls[:2], urls[2:])","title":"Removing non-scrapable urls"},{"location":"development/architecture/#todo","text":"add more useful examples","title":"TODO"},{"location":"development/architecture/#creation","text":"You can create a new scraper in order to support scraping for new sites. More details about this in \"creating a scraper\"","title":"Creation"},{"location":"development/architecture/#crawler","text":"TODO","title":"Crawler"},{"location":"development/architecture/#creation_1","text":"You can create a new crawler in order to support exploring new urls for new sites. More details about this in \"creating a crawler\"","title":"Creation"},{"location":"development/architecture/#persistency-manager","text":"TODO","title":"Persistency Manager"},{"location":"development/architecture/#creation_2","text":"You can create a new Persistency Manager object in order to support new ways of storing data. More details about this in \"creating a persistency manager\"","title":"Creation"},{"location":"development/architecture/#experiment","text":"TODO","title":"Experiment"},{"location":"development/architecture/#experimentfsmanager","text":"TODO","title":"ExperimentFSManager"},{"location":"development/creating-a-crawler/","text":"Creating a Crawler TODO","title":"Creating a Crawler"},{"location":"development/creating-a-crawler/#creating-a-crawler","text":"TODO","title":"Creating a Crawler"},{"location":"development/creating-a-persistency-manager/","text":"Creating a Persistency Manager The Persistency manager is an object required by the high level manager object microscope.Manager in order to access the data in which it will work. Such data can be obtained in many ways and stored in even more ways. So, in order to support multiple storage alternatives, we provide the BasePersistencyManager class, an interface that should be implemented by any Persistency Manager so it can be used to retrieve data, and pass it to the microscope.Manager to tell it how to access the desired data. In this page we will describe how to create your own manager. In the following example we will implement a dict-based persistency manager. Implementing a new Persistency Manager class First thing first, we have to import the base class that provides the interface to be implemented. from c4v.scraper.persistency_manager import BasePersistencyManager class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" Warning Remember always document the class to have a bit more context about what it does The only required methods to be implemented are the ones marked with NotImplementedError , which (for now) are actually all of them. Constructor: First we add the required initialization code in the constructor, don't forget to call the parent constructor: from typing import Dict from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" def __init__(self): super().__init__() # Don't forget to call the parent class constructor # We will store ScrapedData instances in a dict, using its # url as a key _stored_data : Dict[str, ScrapedData] = {} Get All This function is your main interface function to access stored data. Read carefully the description in the BasePersistencyManager class to know what is expected for this function to do. Return an iterator over the set of stored instances Parameters : + limit : int = Max amount of elements to retrieve. If negative, no limit is assumed. + scraped : bool = True if retrieved data should be scraped, false if it shouldn't, None if not relevant + order_by : str = (optional) names of the fields to use for sorting, first char should be order, - for descending, + for ascending, following chars in each string should be a valid name of a field in the ScrapedData dataclass. If no provided, no order is ensured Return : Iterator of stored ScrapedData instances We start by creating the function and adding some sanity check. Remember that this is possibly going to be useful in the future from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... def get_all(self, limit: int, scraped: bool, order_by: List[str] = None) -> Iterator[ScrapedData]: # Remember to do some sanity check valid_fields = {f.name for f in fields(ScrapedData)} order_by = order_by or [] for order in order_by: if not (order and order[0] in [\"-\", \"+\"] and order[1:] in valid_fields): raise ValueError(\"not valid order provided: \" + order) Now we retrieve and filter the stored data: from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... def get_all(self, limit: int, scraped: bool, order_by: List[str] = None) -> Iterator[ScrapedData]: # Sanity check... # Get actual data, filtering by scraped or not # This lambda is checking if the data instance should be included in the query. # We assume that an instance is not scraped when its last_scraped field is not provided # (AKA it wasn't scraped in any moment) goes_in = lambda x: scraped == None or \\ (x.last_scraped and scraped) or \\ (not x.last_scraped and not scraped) data = [d for d in self._stored_data.values() if goes_in(d)] # Now sort it as requested order_by = order_by or [] for field in reversed(order_by): # order in inverse key order so you preserve multi sorting asc = field[0] == \"+\" data.sort(key=lambda d: d.__getattribute__(field[1:]), reverse=not asc) And finally, we take only the requested amount of elements from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... def get_all(self, limit: int, scraped: bool, order_by: List[str] = None) -> Iterator[ScrapedData]: # Sanity check... # Data collecting and ordering... # Set up limit # All elements by default n_elems = len(data) limit = min(limit, n_elems) if limit > 0 else n_elems for i in range(limit): yield data[i] Filter known urls: This function is required in order to check which urls are already in database, scraped or not. This is important to efficiently check if a newly discovered url should be saved in the database. From the function description: Filter out urls that are already known to the database, leaving only the ones that are new from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dat aclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... # get_all... def filter_known_urls(self, urls: List[str]) -> List[str]: # Just return the ones that are not stored in our dict return [url for url in urls if not self._stored_data.get(url)] Filter scraped urls This function is useful when you want to know if a given set of urls is actually already scraped. From the function description: Filter out urls whose data is already known , leaving only the ones to be scraped for first time Parameters : + urls : [str] = List of urls to filter Return : A list of urls such that none of them has been scraped from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... # get_all... # filter_known_urls... def filter_scraped_urls(self, urls: List[str]) -> List[str]: # Just return the ones that are either not stored, or stored but not yet scraped return [ url for url in urls if not self._stored_data.get(url) or\\ not self._stored_data[url].last_scraped ] was scraped This function is useful to tell if a given url corresponds to an actual article that it's scraped and stored in the underlying database (a dict in our case). From the function description: Tells if a given url is already scraped (it's related data is already know) Parameters : + url : str = url to check if it was already scraped Return : If the given url's related data is already known from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... # get_all... # filter_known_urls... # filter_scraped_urls... def was_scraped(self, url: str) -> bool: # Return true if it's stored in DB and it was scraped at some point return bool(self._stored_data.get(url) and self._stored_data[url].last_scraped) save This function is used to add new data to the underlying storage. It will override any existing data. From the function description: Save provided data to underlying storage. If some some urls are already in local storage , override them with provided new data. If not, just add them . Parameters : + data : [ScrapedData] = data to be saved, will override existent data from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... # get_all... # filter_known_urls... # filter_scraped_urls... # was_scraped... def save(self, url_data: List[ScrapedData]): # Add data instance to dict, use its url as key for d in url_data: self._stored_data[d.url] = d Delete This function will remove listed data by url. If some url is not actually stored, it will ignore it. From the function description: Delete provided urls from persistent storage. If does not exists, ignore it. Parameters : urls : [str] = Urls to be deleted from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... # get_all... # filter_known_urls... # filter_scraped_urls... # was_scraped... # save... def delete(self, urls: List[str]): # Just remove these urls from storage dict for url in urls: if self._stored_data.get(url): del self._stored_data[url] Testing Last but not least, if you are working on the library source code, don't forget to test your brand new PersistencyManager object. There's plenty of useful utility functions to easily add testing for a new manager object, but don't forget to add more tests for your specific implementation if it requires it. Creating your test object using conftest You can create an instance in a custom way per function if you like, but this is probably an inneficient way to configure the object to use during testing, specially for the type of object we're trying to test (a database abstraction). For this reason, we encourage you to create the object in the conftest file, allowing you to centralize object configuration in a single function or set of functions that will be reused by your tests. Go to the tests/scraper/conftest.py file Import your new class, in our case: from c4v.scraper.persistency_manager.example_dict_manager import DictManager Add the following code at the end of the file: @pytest.fixture def test_example_manager() -> DictManager: \"\"\" Build a test sqlite manager \"\"\" return DictManager() # Replace with your own manager Creating a testing file Go to the tests/scraper/persistency_manager/ folder from the project root and create a new file for your new persistency manager class, test_example_manager.py in our case. Warning Remember to start every test file with the \"test_\" prefix to every test file. Importing the relevant code As you might expect, we have to import our new class to test it properly. Besides that, we should import the utility functions that you can use to easily test most essential operations in a persistency manager from c4v.scraper.persistency_manager.example_dict_manager import DictManager # Replace here with your new class # These are the functions you will use to to # test your new class. Of course, you can and should add # more tests and test functions if your new class requires it. # These are the bare minimum testing functions. from tests.scraper.utils import ( util_test_filter_scraped_urls, util_test_get_in_order, util_test_instance_delete, util_test_order_parsing, util_test_save_for, util_test_save_overrides_for, util_test_url_filtering ) Using these functions Using the imported functions to create the test cases is as easy as just call them in your test case: # imports... def test_save_example_manager(test_example_manager : DictManager): \"\"\" Test if save operation creates a new instance when that one does not exists \"\"\" util_test_save_for(test_example_manager) def test_overrides_example_manager(test_example_manager : DictManager): \"\"\" Test if save an ScrapedData instance overrides existent ones \"\"\" util_test_save_overrides_for(test_example_manager) def test_list_example_manager(test_example_manager : DictManager): \"\"\" Test listing of ScrapedData instances \"\"\" util_test_filter_scraped_urls(test_example_manager) def test_filter_url_lists(test_example_manager : DictManager): \"\"\" Test filtering of ScrapedData instances \"\"\" util_test_url_filtering(test_example_manager) def test_delete_row(test_example_manager : DictManager): \"\"\" Check that deletion works properly \"\"\" util_test_instance_delete(test_example_manager) def test_get_in_order(test_example_manager : DictManager): \"\"\" Check that ordering works properly in get_all function \"\"\" util_test_get_in_order(test_example_manager) def test_order_parsing(test_example_manager : DictManager): \"\"\" Check that invalid formats for ordering are handled with ValueError Exceptions \"\"\" util_test_order_parsing(test_example_manager) As this is a lot of boilerplate, feel free to copy and paste and replace the example_manager part to fit your case. And that's it! You have a new persistency manager object ready to go. You can find this entire example in the following files: src/c4v/scraper/persistency_manager/example_dict_manager.py : Example class implementation. tests/scraper/persistency_manager/test_example_manager.py : Example class testing suite. tests/scraper/conftest.py : Where to find the configured example object","title":"Creating a Persistency Manager"},{"location":"development/creating-a-persistency-manager/#creating-a-persistency-manager","text":"The Persistency manager is an object required by the high level manager object microscope.Manager in order to access the data in which it will work. Such data can be obtained in many ways and stored in even more ways. So, in order to support multiple storage alternatives, we provide the BasePersistencyManager class, an interface that should be implemented by any Persistency Manager so it can be used to retrieve data, and pass it to the microscope.Manager to tell it how to access the desired data. In this page we will describe how to create your own manager. In the following example we will implement a dict-based persistency manager.","title":"Creating a Persistency Manager"},{"location":"development/creating-a-persistency-manager/#implementing-a-new-persistency-manager-class","text":"First thing first, we have to import the base class that provides the interface to be implemented. from c4v.scraper.persistency_manager import BasePersistencyManager class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" Warning Remember always document the class to have a bit more context about what it does The only required methods to be implemented are the ones marked with NotImplementedError , which (for now) are actually all of them.","title":"Implementing a new Persistency Manager class"},{"location":"development/creating-a-persistency-manager/#constructor","text":"First we add the required initialization code in the constructor, don't forget to call the parent constructor: from typing import Dict from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" def __init__(self): super().__init__() # Don't forget to call the parent class constructor # We will store ScrapedData instances in a dict, using its # url as a key _stored_data : Dict[str, ScrapedData] = {}","title":"Constructor:"},{"location":"development/creating-a-persistency-manager/#get-all","text":"This function is your main interface function to access stored data. Read carefully the description in the BasePersistencyManager class to know what is expected for this function to do. Return an iterator over the set of stored instances Parameters : + limit : int = Max amount of elements to retrieve. If negative, no limit is assumed. + scraped : bool = True if retrieved data should be scraped, false if it shouldn't, None if not relevant + order_by : str = (optional) names of the fields to use for sorting, first char should be order, - for descending, + for ascending, following chars in each string should be a valid name of a field in the ScrapedData dataclass. If no provided, no order is ensured Return : Iterator of stored ScrapedData instances We start by creating the function and adding some sanity check. Remember that this is possibly going to be useful in the future from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... def get_all(self, limit: int, scraped: bool, order_by: List[str] = None) -> Iterator[ScrapedData]: # Remember to do some sanity check valid_fields = {f.name for f in fields(ScrapedData)} order_by = order_by or [] for order in order_by: if not (order and order[0] in [\"-\", \"+\"] and order[1:] in valid_fields): raise ValueError(\"not valid order provided: \" + order) Now we retrieve and filter the stored data: from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... def get_all(self, limit: int, scraped: bool, order_by: List[str] = None) -> Iterator[ScrapedData]: # Sanity check... # Get actual data, filtering by scraped or not # This lambda is checking if the data instance should be included in the query. # We assume that an instance is not scraped when its last_scraped field is not provided # (AKA it wasn't scraped in any moment) goes_in = lambda x: scraped == None or \\ (x.last_scraped and scraped) or \\ (not x.last_scraped and not scraped) data = [d for d in self._stored_data.values() if goes_in(d)] # Now sort it as requested order_by = order_by or [] for field in reversed(order_by): # order in inverse key order so you preserve multi sorting asc = field[0] == \"+\" data.sort(key=lambda d: d.__getattribute__(field[1:]), reverse=not asc) And finally, we take only the requested amount of elements from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... def get_all(self, limit: int, scraped: bool, order_by: List[str] = None) -> Iterator[ScrapedData]: # Sanity check... # Data collecting and ordering... # Set up limit # All elements by default n_elems = len(data) limit = min(limit, n_elems) if limit > 0 else n_elems for i in range(limit): yield data[i]","title":"Get All"},{"location":"development/creating-a-persistency-manager/#filter-known-urls","text":"This function is required in order to check which urls are already in database, scraped or not. This is important to efficiently check if a newly discovered url should be saved in the database. From the function description: Filter out urls that are already known to the database, leaving only the ones that are new from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dat aclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... # get_all... def filter_known_urls(self, urls: List[str]) -> List[str]: # Just return the ones that are not stored in our dict return [url for url in urls if not self._stored_data.get(url)]","title":"Filter known urls:"},{"location":"development/creating-a-persistency-manager/#filter-scraped-urls","text":"This function is useful when you want to know if a given set of urls is actually already scraped. From the function description: Filter out urls whose data is already known , leaving only the ones to be scraped for first time Parameters : + urls : [str] = List of urls to filter Return : A list of urls such that none of them has been scraped from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... # get_all... # filter_known_urls... def filter_scraped_urls(self, urls: List[str]) -> List[str]: # Just return the ones that are either not stored, or stored but not yet scraped return [ url for url in urls if not self._stored_data.get(url) or\\ not self._stored_data[url].last_scraped ]","title":"Filter scraped urls"},{"location":"development/creating-a-persistency-manager/#was-scraped","text":"This function is useful to tell if a given url corresponds to an actual article that it's scraped and stored in the underlying database (a dict in our case). From the function description: Tells if a given url is already scraped (it's related data is already know) Parameters : + url : str = url to check if it was already scraped Return : If the given url's related data is already known from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... # get_all... # filter_known_urls... # filter_scraped_urls... def was_scraped(self, url: str) -> bool: # Return true if it's stored in DB and it was scraped at some point return bool(self._stored_data.get(url) and self._stored_data[url].last_scraped)","title":"was scraped"},{"location":"development/creating-a-persistency-manager/#save","text":"This function is used to add new data to the underlying storage. It will override any existing data. From the function description: Save provided data to underlying storage. If some some urls are already in local storage , override them with provided new data. If not, just add them . Parameters : + data : [ScrapedData] = data to be saved, will override existent data from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... # get_all... # filter_known_urls... # filter_scraped_urls... # was_scraped... def save(self, url_data: List[ScrapedData]): # Add data instance to dict, use its url as key for d in url_data: self._stored_data[d.url] = d","title":"save"},{"location":"development/creating-a-persistency-manager/#delete","text":"This function will remove listed data by url. If some url is not actually stored, it will ignore it. From the function description: Delete provided urls from persistent storage. If does not exists, ignore it. Parameters : urls : [str] = Urls to be deleted from typing import Dict, Iterator, List from c4v.scraper.persistency_manager import BasePersistencyManager from c4v.scraper import ScrapedData from dataclasses import fields class DictManager(BasePersistencyManager): \"\"\" Persistency Manager class that stores data in a dict object \"\"\" # __init__... # get_all... # filter_known_urls... # filter_scraped_urls... # was_scraped... # save... def delete(self, urls: List[str]): # Just remove these urls from storage dict for url in urls: if self._stored_data.get(url): del self._stored_data[url]","title":"Delete"},{"location":"development/creating-a-persistency-manager/#testing","text":"Last but not least, if you are working on the library source code, don't forget to test your brand new PersistencyManager object. There's plenty of useful utility functions to easily add testing for a new manager object, but don't forget to add more tests for your specific implementation if it requires it.","title":"Testing"},{"location":"development/creating-a-persistency-manager/#creating-your-test-object-using-conftest","text":"You can create an instance in a custom way per function if you like, but this is probably an inneficient way to configure the object to use during testing, specially for the type of object we're trying to test (a database abstraction). For this reason, we encourage you to create the object in the conftest file, allowing you to centralize object configuration in a single function or set of functions that will be reused by your tests. Go to the tests/scraper/conftest.py file Import your new class, in our case: from c4v.scraper.persistency_manager.example_dict_manager import DictManager Add the following code at the end of the file: @pytest.fixture def test_example_manager() -> DictManager: \"\"\" Build a test sqlite manager \"\"\" return DictManager() # Replace with your own manager","title":"Creating your test object using conftest"},{"location":"development/creating-a-persistency-manager/#creating-a-testing-file","text":"Go to the tests/scraper/persistency_manager/ folder from the project root and create a new file for your new persistency manager class, test_example_manager.py in our case. Warning Remember to start every test file with the \"test_\" prefix to every test file.","title":"Creating a testing file"},{"location":"development/creating-a-persistency-manager/#importing-the-relevant-code","text":"As you might expect, we have to import our new class to test it properly. Besides that, we should import the utility functions that you can use to easily test most essential operations in a persistency manager from c4v.scraper.persistency_manager.example_dict_manager import DictManager # Replace here with your new class # These are the functions you will use to to # test your new class. Of course, you can and should add # more tests and test functions if your new class requires it. # These are the bare minimum testing functions. from tests.scraper.utils import ( util_test_filter_scraped_urls, util_test_get_in_order, util_test_instance_delete, util_test_order_parsing, util_test_save_for, util_test_save_overrides_for, util_test_url_filtering )","title":"Importing the relevant code"},{"location":"development/creating-a-persistency-manager/#using-these-functions","text":"Using the imported functions to create the test cases is as easy as just call them in your test case: # imports... def test_save_example_manager(test_example_manager : DictManager): \"\"\" Test if save operation creates a new instance when that one does not exists \"\"\" util_test_save_for(test_example_manager) def test_overrides_example_manager(test_example_manager : DictManager): \"\"\" Test if save an ScrapedData instance overrides existent ones \"\"\" util_test_save_overrides_for(test_example_manager) def test_list_example_manager(test_example_manager : DictManager): \"\"\" Test listing of ScrapedData instances \"\"\" util_test_filter_scraped_urls(test_example_manager) def test_filter_url_lists(test_example_manager : DictManager): \"\"\" Test filtering of ScrapedData instances \"\"\" util_test_url_filtering(test_example_manager) def test_delete_row(test_example_manager : DictManager): \"\"\" Check that deletion works properly \"\"\" util_test_instance_delete(test_example_manager) def test_get_in_order(test_example_manager : DictManager): \"\"\" Check that ordering works properly in get_all function \"\"\" util_test_get_in_order(test_example_manager) def test_order_parsing(test_example_manager : DictManager): \"\"\" Check that invalid formats for ordering are handled with ValueError Exceptions \"\"\" util_test_order_parsing(test_example_manager) As this is a lot of boilerplate, feel free to copy and paste and replace the example_manager part to fit your case. And that's it! You have a new persistency manager object ready to go. You can find this entire example in the following files: src/c4v/scraper/persistency_manager/example_dict_manager.py : Example class implementation. tests/scraper/persistency_manager/test_example_manager.py : Example class testing suite. tests/scraper/conftest.py : Where to find the configured example object","title":"Using these functions"},{"location":"development/creating-a-scraper/","text":"Creating a Scraper TODO","title":"Creating a Scraper"},{"location":"development/creating-a-scraper/#creating-a-scraper","text":"TODO","title":"Creating a Scraper"},{"location":"usage/microscope-as-a-library/","text":"Microscope as a Library You can use Microscope as a library in many ways using buth its API and its core components. Using the high level Api The main object you can use to access common operations for the Microscope library is the microscope.Manager object. For example, here you can use it to crawl for urls in some known site: import c4v.microscope as ms # creates a manager object manager = ms.Manager.from_default() # crawl new urls from the internet d = manager.crawl_new_urls_for( [\"primicia\"], # Name of crawlers to use when crawling limit=10 # Maximum ammount of urls to crawl ) print(d) # A (possibly empty) list of urls as string print(len(d)) # a number <= 10 Note You can find which crawler names are available for you to use using manager.get_available_crawlers() Examples The following are some examples for some common use cases Scraping and crawling at the same time The following code will crawl and scrape 10 urls from primicia's website import c4v.microscope as ms # creates a manager object manager = ms.Manager.from_default() # crawl new urls from the internet d = manager.crawl_and_scrape_for( [\"primicia\"], # scrape for primicia limit=10 # up to ten urls ) print(d) # bunch of text probably, instances of ScrapedData class print(len(d)) # amount of scraped data instances, <= 10 as we requested limit = 10 Get data for known urls This is probably the most common operation you may want to perform, retrieving data for urls you want to process using this library import c4v.microscope as ms # creates a manager object manager = ms.Manager.from_default() # Try to get data for this urls d = manager.get_bulk_data_for( [ \"https://primicia.com.ve/deportes/emiliano-martinez-y-buendia-dejan-la-seleccion-argentina/\", \"https://primicia.com.ve/deportes/odubel-remolco-la-de-ganar-en-el-decimo/\", \"https://primicia.com.ve/deportes/valtteri-bottas-le-dice-adios-a-mercedes-e-ira-a-alfa-romeo/\" ] ) print(d) # data for the three given urls Using Local Storage You can provide a database manager to store data scraped with the microscope manager locally, here we will see some examples using an SQLite database Get data for known urls This example is the same as before, but now we will store the results directly in the database, so we can use that data afterwards without having to scrape them. import c4v.microscope as ms from datetime import datetime # creates a manager object manager = ms.Manager.from_local_sqlite_db(\"test_db.sqlite\") # will create a file to store retrieved data # Measure time before storing start = datetime.now() d = manager.get_bulk_data_for( [ \"https://primicia.com.ve/deportes/emiliano-martinez-y-buendia-dejan-la-seleccion-argentina/\", \"https://primicia.com.ve/deportes/odubel-remolco-la-de-ganar-en-el-decimo/\", \"https://primicia.com.ve/deportes/valtteri-bottas-le-dice-adios-a-mercedes-e-ira-a-alfa-romeo/\" ] ) end = datetime.now() print(\"before: \", (end - start).total_seconds()) # 2.137678, May vary depending on your internet connection # Measure time after storing start = datetime.now() d = manager.get_bulk_data_for( [ \"https://primicia.com.ve/deportes/emiliano-martinez-y-buendia-dejan-la-seleccion-argentina/\", \"https://primicia.com.ve/deportes/odubel-remolco-la-de-ganar-en-el-decimo/\", \"https://primicia.com.ve/deportes/valtteri-bottas-le-dice-adios-a-mercedes-e-ira-a-alfa-romeo/\" ] ) end = datetime.now() print(\"after: \", (end - start).total_seconds()) # 0.000406 Note If you don't provide any db name, calling the manager constructor as Manager.from_local_sqlite_db() you can use the library db, where data required by the CLI tool is stored by default, this way you can browse data and process it using code Retrieving data from a local db Once you have scraped & stored data using the local SQLite manager, you may want to retrieve for further processing. You can do so by using the get_all function that returns all stored ScrapedData instances: import c4v.microscope as ms # creates a manager object manager = ms.Manager.from_local_sqlite_db(\"test_db.sqlite\") for d in manager.get_all(): print(d) # prints the three instances scraped in the previous example Using your own database implementation You might be interested in using your own persistency management strategy, you can do so by implementing the BasePersistencyManager class. For example, let's say this is our implementation: class MyDBManager(BasePersistencyManager): \"\"\" Store data in the class itself \"\"\" data = set() def get_all(self, limit = 100, scraped = None): limit = 10000 if limit < 0 else limit # very high number when negative number is provided def goes_in(scraped_data): was_scraped = self.was_scraped(scraped_data.url) return (scraped and was_scraped) or (scraped == False and not was_scraped) or (scraped == None) return (d for d in list(MyDBManager.data)[:limit] if goes_in(d)) def was_scraped(self, url): return any(d.last_scraped != None and d.url == url for d in MyDBManager.data) def save(self, url_data): MyDBManager.data = MyDBManager.data.union(url_data) def filter_scraped_urls(self, urls): scraped_urls = { d.url for d in MyDBManager.data if self.was_scraped(d.url) } return [url for url in urls if url not in scraped_urls] This is a partial implementation, with the minimum code to save and retrieve data, let's use it as a storage backend for the manager class. We will use the same example as before, but with our new backend: import c4v.microscope as ms from datetime import datetime # creates a manager object manager = ms.Manager(MyDBManager()) urls = [ \"https://primicia.com.ve/deportes/emiliano-martinez-y-buendia-dejan-la-seleccion-argentina/\", \"https://primicia.com.ve/deportes/odubel-remolco-la-de-ganar-en-el-decimo/\", \"https://primicia.com.ve/deportes/valtteri-bottas-le-dice-adios-a-mercedes-e-ira-a-alfa-romeo/\" ] # Measure time before storing start = datetime.now() d = manager.get_bulk_data_for(urls) end = datetime.now() print(\"before: \", (end - start).total_seconds()) # 2.155265s, scraped from internet # Measure time after storing start = datetime.now() d = manager.get_bulk_data_for(urls) end = datetime.now() print(\"after: \", (end - start).total_seconds()) # 1.7e-05s, retrieved from local storage Warning Please not that this is not a full implementation , and thus, it can't be used with the microscope.Manager object as a database backend. If you need to do so, follow the instructions in this page. Using the Low Level Api If you need a more fine-grained control, you can use the primary components of the microscope library, importing the following modules: c4v.scraper : Functions to scrape data from the internet for a given set of urls c4v.scraper.crawlers : Classes for crawling and implement a new crawler c4v.scraper.persistency_manager : Classes for storing data locally and implement a new persistency manager c4v.classifier : Classes for classifier and its experiments: Classifier Class Classifier experiment class Experiment Base class, if you want to create more experiments that will use the same filesystem as the rest of the experiments More about this in the next section","title":"Microscope as a Library"},{"location":"usage/microscope-as-a-library/#microscope-as-a-library","text":"You can use Microscope as a library in many ways using buth its API and its core components.","title":"Microscope as a Library"},{"location":"usage/microscope-as-a-library/#using-the-high-level-api","text":"The main object you can use to access common operations for the Microscope library is the microscope.Manager object. For example, here you can use it to crawl for urls in some known site: import c4v.microscope as ms # creates a manager object manager = ms.Manager.from_default() # crawl new urls from the internet d = manager.crawl_new_urls_for( [\"primicia\"], # Name of crawlers to use when crawling limit=10 # Maximum ammount of urls to crawl ) print(d) # A (possibly empty) list of urls as string print(len(d)) # a number <= 10 Note You can find which crawler names are available for you to use using manager.get_available_crawlers()","title":"Using the high level Api"},{"location":"usage/microscope-as-a-library/#examples","text":"The following are some examples for some common use cases","title":"Examples"},{"location":"usage/microscope-as-a-library/#scraping-and-crawling-at-the-same-time","text":"The following code will crawl and scrape 10 urls from primicia's website import c4v.microscope as ms # creates a manager object manager = ms.Manager.from_default() # crawl new urls from the internet d = manager.crawl_and_scrape_for( [\"primicia\"], # scrape for primicia limit=10 # up to ten urls ) print(d) # bunch of text probably, instances of ScrapedData class print(len(d)) # amount of scraped data instances, <= 10 as we requested limit = 10","title":"Scraping and crawling at the same time"},{"location":"usage/microscope-as-a-library/#get-data-for-known-urls","text":"This is probably the most common operation you may want to perform, retrieving data for urls you want to process using this library import c4v.microscope as ms # creates a manager object manager = ms.Manager.from_default() # Try to get data for this urls d = manager.get_bulk_data_for( [ \"https://primicia.com.ve/deportes/emiliano-martinez-y-buendia-dejan-la-seleccion-argentina/\", \"https://primicia.com.ve/deportes/odubel-remolco-la-de-ganar-en-el-decimo/\", \"https://primicia.com.ve/deportes/valtteri-bottas-le-dice-adios-a-mercedes-e-ira-a-alfa-romeo/\" ] ) print(d) # data for the three given urls","title":"Get data for known urls"},{"location":"usage/microscope-as-a-library/#using-local-storage","text":"You can provide a database manager to store data scraped with the microscope manager locally, here we will see some examples using an SQLite database","title":"Using Local Storage"},{"location":"usage/microscope-as-a-library/#get-data-for-known-urls_1","text":"This example is the same as before, but now we will store the results directly in the database, so we can use that data afterwards without having to scrape them. import c4v.microscope as ms from datetime import datetime # creates a manager object manager = ms.Manager.from_local_sqlite_db(\"test_db.sqlite\") # will create a file to store retrieved data # Measure time before storing start = datetime.now() d = manager.get_bulk_data_for( [ \"https://primicia.com.ve/deportes/emiliano-martinez-y-buendia-dejan-la-seleccion-argentina/\", \"https://primicia.com.ve/deportes/odubel-remolco-la-de-ganar-en-el-decimo/\", \"https://primicia.com.ve/deportes/valtteri-bottas-le-dice-adios-a-mercedes-e-ira-a-alfa-romeo/\" ] ) end = datetime.now() print(\"before: \", (end - start).total_seconds()) # 2.137678, May vary depending on your internet connection # Measure time after storing start = datetime.now() d = manager.get_bulk_data_for( [ \"https://primicia.com.ve/deportes/emiliano-martinez-y-buendia-dejan-la-seleccion-argentina/\", \"https://primicia.com.ve/deportes/odubel-remolco-la-de-ganar-en-el-decimo/\", \"https://primicia.com.ve/deportes/valtteri-bottas-le-dice-adios-a-mercedes-e-ira-a-alfa-romeo/\" ] ) end = datetime.now() print(\"after: \", (end - start).total_seconds()) # 0.000406 Note If you don't provide any db name, calling the manager constructor as Manager.from_local_sqlite_db() you can use the library db, where data required by the CLI tool is stored by default, this way you can browse data and process it using code","title":"Get data for known urls"},{"location":"usage/microscope-as-a-library/#retrieving-data-from-a-local-db","text":"Once you have scraped & stored data using the local SQLite manager, you may want to retrieve for further processing. You can do so by using the get_all function that returns all stored ScrapedData instances: import c4v.microscope as ms # creates a manager object manager = ms.Manager.from_local_sqlite_db(\"test_db.sqlite\") for d in manager.get_all(): print(d) # prints the three instances scraped in the previous example","title":"Retrieving data from a local db"},{"location":"usage/microscope-as-a-library/#using-your-own-database-implementation","text":"You might be interested in using your own persistency management strategy, you can do so by implementing the BasePersistencyManager class. For example, let's say this is our implementation: class MyDBManager(BasePersistencyManager): \"\"\" Store data in the class itself \"\"\" data = set() def get_all(self, limit = 100, scraped = None): limit = 10000 if limit < 0 else limit # very high number when negative number is provided def goes_in(scraped_data): was_scraped = self.was_scraped(scraped_data.url) return (scraped and was_scraped) or (scraped == False and not was_scraped) or (scraped == None) return (d for d in list(MyDBManager.data)[:limit] if goes_in(d)) def was_scraped(self, url): return any(d.last_scraped != None and d.url == url for d in MyDBManager.data) def save(self, url_data): MyDBManager.data = MyDBManager.data.union(url_data) def filter_scraped_urls(self, urls): scraped_urls = { d.url for d in MyDBManager.data if self.was_scraped(d.url) } return [url for url in urls if url not in scraped_urls] This is a partial implementation, with the minimum code to save and retrieve data, let's use it as a storage backend for the manager class. We will use the same example as before, but with our new backend: import c4v.microscope as ms from datetime import datetime # creates a manager object manager = ms.Manager(MyDBManager()) urls = [ \"https://primicia.com.ve/deportes/emiliano-martinez-y-buendia-dejan-la-seleccion-argentina/\", \"https://primicia.com.ve/deportes/odubel-remolco-la-de-ganar-en-el-decimo/\", \"https://primicia.com.ve/deportes/valtteri-bottas-le-dice-adios-a-mercedes-e-ira-a-alfa-romeo/\" ] # Measure time before storing start = datetime.now() d = manager.get_bulk_data_for(urls) end = datetime.now() print(\"before: \", (end - start).total_seconds()) # 2.155265s, scraped from internet # Measure time after storing start = datetime.now() d = manager.get_bulk_data_for(urls) end = datetime.now() print(\"after: \", (end - start).total_seconds()) # 1.7e-05s, retrieved from local storage Warning Please not that this is not a full implementation , and thus, it can't be used with the microscope.Manager object as a database backend. If you need to do so, follow the instructions in this page.","title":"Using your own database implementation"},{"location":"usage/microscope-as-a-library/#using-the-low-level-api","text":"If you need a more fine-grained control, you can use the primary components of the microscope library, importing the following modules: c4v.scraper : Functions to scrape data from the internet for a given set of urls c4v.scraper.crawlers : Classes for crawling and implement a new crawler c4v.scraper.persistency_manager : Classes for storing data locally and implement a new persistency manager c4v.classifier : Classes for classifier and its experiments: Classifier Class Classifier experiment class Experiment Base class, if you want to create more experiments that will use the same filesystem as the rest of the experiments More about this in the next section","title":"Using the Low Level Api"},{"location":"usage/microscope-as-cli/","text":"Microscope as a CLI TODO","title":"Microscope as a command line tool"},{"location":"usage/microscope-as-cli/#microscope-as-a-cli","text":"TODO","title":"Microscope as a CLI"},{"location":"usage/primary-components/","text":"Primary components You can do everything the high level miscroscope.Manager api can by hand using its primary components instead, this way you can have better control if you need it. Scraper TODO Crawler TODO Persistency Manager TODO Classifier TODO","title":"Primary Components"},{"location":"usage/primary-components/#primary-components","text":"You can do everything the high level miscroscope.Manager api can by hand using its primary components instead, this way you can have better control if you need it.","title":"Primary components"},{"location":"usage/primary-components/#scraper","text":"TODO","title":"Scraper"},{"location":"usage/primary-components/#crawler","text":"TODO","title":"Crawler"},{"location":"usage/primary-components/#persistency-manager","text":"TODO","title":"Persistency Manager"},{"location":"usage/primary-components/#classifier","text":"TODO","title":"Classifier"}]}